# VeriAIDPO - H∆∞·ªõng D·∫´n Hu·∫•n Luy·ªán PhoBERT
## C√°ch Hu·∫•n Luy·ªán PhoBERT tr√™n D·ªØ Li·ªáu PDPL 2025 Vi·ªát Nam

### **T√≥m T·∫Øt**

T√†i li·ªáu n√†y cung c·∫•p h∆∞·ªõng d·∫´n chi ti·∫øt t·ª´ng b∆∞·ªõc ƒë·ªÉ hu·∫•n luy·ªán PhoBERT tr√™n b·ªô d·ªØ li·ªáu tu√¢n th·ªß PDPL Vi·ªát Nam. L√†m theo c√°c b∆∞·ªõc n√†y ƒë·ªÉ t·∫°o ra m√¥ h√¨nh ph√¢n lo·∫°i vƒÉn b·∫£n ph√°p l√Ω ti·∫øng Vi·ªát th√†nh 8 nguy√™n t·∫Øc tu√¢n th·ªß PDPL.

---

## **üìã T·ªïng Quan: B·∫°n S·∫Ω X√¢y D·ª±ng G√¨**

**M·ª•c ti√™u**: Hu·∫•n luy·ªán PhoBERT ƒë·ªÉ ph√¢n lo·∫°i vƒÉn b·∫£n PDPL ti·∫øng Vi·ªát v√†o c√°c danh m·ª•c tu√¢n th·ªß

**ƒê·∫ßu v√†o**: VƒÉn b·∫£n ti·∫øng Vi·ªát (VD: "C√¥ng ty ph·∫£i b·∫£o v·ªá d·ªØ li·ªáu c√° nh√¢n")
**ƒê·∫ßu ra**: Danh m·ª•c tu√¢n th·ªß (VD: "T·ªëi thi·ªÉu h√≥a d·ªØ li·ªáu") + ƒë·ªô tin c·∫≠y

**Quy Tr√¨nh Hu·∫•n Luy·ªán**:
```
B∆∞·ªõc 1: Chu·∫©n b·ªã d·ªØ li·ªáu (vƒÉn b·∫£n ph√°p l√Ω PDPL + nh√£n)
   ‚Üì
B∆∞·ªõc 2: Ti·ªÅn x·ª≠ l√Ω v·ªõi VnCoreNLP (t√°ch t·ª´ ti·∫øng Vi·ªát)
   ‚Üì
B∆∞·ªõc 3: Hu·∫•n luy·ªán PhoBERT (h·ªçc chuy·ªÉn giao)
   ‚Üì
B∆∞·ªõc 4: ƒê√°nh gi√° m√¥ h√¨nh (ki·ªÉm tra ƒë·ªô ch√≠nh x√°c)
   ‚Üì
B∆∞·ªõc 5: L∆∞u & tri·ªÉn khai m√¥ h√¨nh
```

---

## **B∆∞·ªõc 1: Chu·∫©n B·ªã D·ªØ Li·ªáu PDPL Vi·ªát Nam**

### **1.1 C·∫•u Tr√∫c D·ªØ Li·ªáu**

T·∫°o d·ªØ li·ªáu hu·∫•n luy·ªán d·∫°ng JSONL (m·ªói d√≤ng 1 ƒë·ªëi t∆∞·ª£ng JSON):

```jsonl
{"text": "C√¥ng ty ph·∫£i thu th·∫≠p d·ªØ li·ªáu m·ªôt c√°ch h·ª£p ph√°p v√† minh b·∫°ch", "label": 0}
{"text": "D·ªØ li·ªáu c√° nh√¢n ch·ªâ ƒë∆∞·ª£c s·ª≠ d·ª•ng cho m·ª•c ƒë√≠ch ƒë√£ th√¥ng b√°o", "label": 1}
{"text": "Ch·ªâ thu th·∫≠p d·ªØ li·ªáu c·∫ßn thi·∫øt cho m·ª•c ƒë√≠ch c·ª• th·ªÉ", "label": 2}
{"text": "D·ªØ li·ªáu ph·∫£i ch√≠nh x√°c v√† ƒë∆∞·ª£c c·∫≠p nh·∫≠t th∆∞·ªùng xuy√™n", "label": 3}
{"text": "Kh√¥ng l∆∞u tr·ªØ d·ªØ li·ªáu l√¢u h∆°n th·ªùi gian c·∫ßn thi·∫øt", "label": 4}
{"text": "D·ªØ li·ªáu ph·∫£i ƒë∆∞·ª£c m√£ h√≥a v√† b·∫£o v·ªá an to√†n", "label": 5}
{"text": "Doanh nghi·ªáp ch·ªãu tr√°ch nhi·ªám v·ªÅ vi·ªác x·ª≠ l√Ω d·ªØ li·ªáu", "label": 6}
{"text": "Ng∆∞·ªùi d√πng c√≥ quy·ªÅn truy c·∫≠p v√† x√≥a d·ªØ li·ªáu c√° nh√¢n", "label": 7}
```

### **1.2 √Ånh X·∫° Nh√£n (8 Nguy√™n T·∫Øc PDPL Vi·ªát Nam)**

**Quan tr·ªçng**: S·ª≠ d·ª•ng ti·∫øng Vi·ªát l√†m ng√¥n ng·ªØ ch√≠nh, ti·∫øng Anh l√†m ph·ª•.

```python
# label_mapping.py
"""
Nh√£n Danh M·ª•c Tu√¢n Th·ªß PDPL 2025 Vi·ªát Nam (Song Ng·ªØ)
"""

# Nh√£n ti·∫øng Vi·ªát (s·ª≠ d·ª•ng ch√≠nh trong VeriPortal)
PDPL_NHAN_VI = {
    0: "T√≠nh h·ª£p ph√°p, c√¥ng b·∫±ng v√† minh b·∫°ch",
    1: "H·∫°n ch·∫ø m·ª•c ƒë√≠ch",
    2: "T·ªëi thi·ªÉu h√≥a d·ªØ li·ªáu",
    3: "T√≠nh ch√≠nh x√°c",
    4: "H·∫°n ch·∫ø l∆∞u tr·ªØ",
    5: "T√≠nh to√†n v·∫πn v√† b·∫£o m·∫≠t",
    6: "Tr√°ch nhi·ªám gi·∫£i tr√¨nh",
    7: "Quy·ªÅn c·ªßa ch·ªß th·ªÉ d·ªØ li·ªáu"
}

# Nh√£n ti·∫øng Anh (s·ª≠ d·ª•ng ph·ª• cho b√°o c√°o qu·ªëc t·∫ø)
PDPL_NHAN_EN = {
    0: "Lawfulness, fairness and transparency",
    1: "Purpose limitation",
    2: "Data minimization",
    3: "Accuracy",
    4: "Storage limitation",
    5: "Integrity and confidentiality",
    6: "Accountability",
    7: "Rights of data subjects"
}

# S·ª≠ d·ª•ng ti·∫øng Vi·ªát l√†m m·∫∑c ƒë·ªãnh (∆∞u ti√™n vƒÉn h√≥a Vi·ªát Nam)
PDPL_NHAN = PDPL_NHAN_VI

# √Ånh x·∫° ng∆∞·ª£c
NHAN_TO_ID = {v: k for k, v in PDPL_NHAN.items()}

# H√†m l·∫•y nh√£n theo ng√¥n ng·ªØ
def lay_nhan(ma_nhan, ngon_ngu='vi'):
    """
    L·∫•y nh√£n PDPL theo ng√¥n ng·ªØ
    
    Tham s·ªë:
        ma_nhan (int): M√£ nh√£n (0-7)
        ngon_ngu (str): 'vi' cho ti·∫øng Vi·ªát (m·∫∑c ƒë·ªãnh), 'en' cho ti·∫øng Anh
    
    Tr·∫£ v·ªÅ:
        str: Nh√£n theo ng√¥n ng·ªØ ƒë√£ ch·ªçn
    """
    if ngon_ngu == 'en':
        return PDPL_NHAN_EN[ma_nhan]
    else:
        return PDPL_NHAN_VI[ma_nhan]
```

**T·∫°i sao thi·∫øt k·∫ø nh∆∞ v·∫≠y?**

1. ‚úÖ **Ti·∫øng Vi·ªát l√† ng√¥n ng·ªØ ch√≠nh** ‚Üí Ph√π h·ª£p v·ªõi vƒÉn h√≥a v√† ng∆∞·ªùi d√πng Vi·ªát Nam
2. ‚úÖ **Ti·∫øng Anh l√† ng√¥n ng·ªØ ph·ª•** ‚Üí H·ªó tr·ª£ b√°o c√°o qu·ªëc t·∫ø
3. ‚úÖ **D·ªØ li·ªáu hu·∫•n luy·ªán b·∫±ng ti·∫øng Vi·ªát** ‚Üí PhoBERT h·ªçc ng√¥n ng·ªØ ph√°p l√Ω Vi·ªát Nam
4. ‚úÖ **T√¥n tr·ªçng ƒëa d·∫°ng v√πng mi·ªÅn** ‚Üí H·ªó tr·ª£ ti·∫øng Vi·ªát B·∫Øc, Trung, Nam

### **1.3 T·∫°o T·ªáp Hu·∫•n Luy·ªán**

T·∫°o ba t·ªáp trong th∆∞ m·ª•c `du_lieu/`:

```
du_lieu/
‚îú‚îÄ‚îÄ huan_luyen.jsonl     (70% d·ªØ li·ªáu, VD: 700 m·∫´u)
‚îú‚îÄ‚îÄ kiem_tra.jsonl       (15% d·ªØ li·ªáu, VD: 150 m·∫´u)
‚îî‚îÄ‚îÄ danh_gia.jsonl       (15% d·ªØ li·ªáu, VD: 150 m·∫´u)
```

**K√≠ch th∆∞·ªõc d·ªØ li·ªáu t·ªëi thi·ªÉu**: 500-1000 m·∫´u cho k·∫øt qu·∫£ t·ªët
**Khuy·∫øn ngh·ªã**: 2000+ m·∫´u cho ch·∫•t l∆∞·ª£ng s·∫£n xu·∫•t

### **1.4 ƒêa D·∫°ng V√πng Mi·ªÅn Vi·ªát Nam**

**Quan tr·ªçng**: Thu th·∫≠p d·ªØ li·ªáu t·ª´ c·∫£ 3 mi·ªÅn ƒë·ªÉ m√¥ h√¨nh hi·ªÉu ƒë∆∞·ª£c s·ª± ƒëa d·∫°ng ng√¥n ng·ªØ:

```python
# V√≠ d·ª• ƒëa d·∫°ng v√πng mi·ªÅn
du_lieu_mien = {
    'Mi·ªÅn B·∫Øc': [
        "Doanh nghi·ªáp ph·∫£i ƒë·∫£m b·∫£o d·ªØ li·ªáu ƒë∆∞·ª£c b·∫£o m·∫≠t",
        "C√¥ng ty c·∫ßn tu√¢n th·ªß quy ƒë·ªãnh v·ªÅ b·∫£o v·ªá d·ªØ li·ªáu c√° nh√¢n"
    ],
    'Mi·ªÅn Trung': [
        "Doanh nghi·ªáp ph·∫£i b·∫£o ƒë·∫£m d·ªØ li·ªáu ƒë∆∞·ª£c b·∫£o m·∫≠t",
        "C√¥ng ty c·∫ßn tu√¢n th·ªß quy ƒë·ªãnh b·∫£o v·ªá d·ªØ li·ªáu c√° nh√¢n"
    ],
    'Mi·ªÅn Nam': [
        "Doanh nghi·ªáp ph·∫£i ƒë·∫£m b·∫£o d·ªØ li·ªáu ƒë∆∞·ª£c b·∫£o m·∫≠t",
        "C√¥ng ty c·∫ßn tu√¢n th·ªß quy ƒë·ªãnh v·ªÅ b·∫£o v·ªá d·ªØ li·ªáu c√° nh√¢n"
    ]
}
```

**L∆∞u √Ω ng√¥n ng·ªØ v√πng mi·ªÅn**:
- **Mi·ªÅn B·∫Øc**: "C·∫ßn ph·∫£i", "ƒë·∫£m b·∫£o", "quy ƒë·ªãnh v·ªÅ"
- **Mi·ªÅn Trung**: "C·∫ßn", "b·∫£o ƒë·∫£m", "quy ƒë·ªãnh"  
- **Mi·ªÅn Nam**: "C·∫ßn", "ƒë·∫£m b·∫£o", "quy ƒë·ªãnh v·ªÅ"

---

## **‚ùì C√¢u H·ªèi Th∆∞·ªùng G·∫∑p: T·∫°i Sao M√£ Nh√£n D√πng S·ªë Thay V√¨ Ch·ªØ?**

### **C√¢u h·ªèi:**
> "T·∫°i sao `label: 0` thay v√¨ `label: "T√≠nh h·ª£p ph√°p"`?"

### **Tr·∫£ l·ªùi:**

**VƒÉn b·∫£n hu·∫•n luy·ªán** (vƒÉn b·∫£n PhoBERT h·ªçc) l√† **ti·∫øng Vi·ªát**:
```jsonl
{"text": "C√¥ng ty ph·∫£i b·∫£o v·ªá d·ªØ li·ªáu c√° nh√¢n", "label": 5}  ‚Üê VƒÉn b·∫£n ti·∫øng Vi·ªát
```

**Nh√£n** ch·ªâ l√† **m√£ s·ªë** (0-7) ƒë·∫°i di·ªán cho c√°c danh m·ª•c:

| L√Ω do | Gi·∫£i th√≠ch |
|-------|------------|
| üîß **K·ªπ thu·∫≠t ML** | C√°c th∆∞ vi·ªán ML (PyTorch, scikit-learn) x·ª≠ l√Ω s·ªë nhanh h∆°n ch·ªØ |
| üíæ **Ti·∫øt ki·ªám b·ªô nh·ªõ** | S·ªë 0-7 (1 byte) < Chu·ªói "T√≠nh h·ª£p ph√°p..." (40 bytes) |
| üåç **T∆∞∆°ng th√≠ch qu·ªëc t·∫ø** | D·ªÖ so s√°nh v·ªõi nghi√™n c·ª©u GDPR to√†n c·∫ßu |
| üéØ **Linh ho·∫°t hi·ªÉn th·ªã** | D·ªÖ d√†ng chuy·ªÉn ƒë·ªïi gi·ªØa ti·∫øng Vi·ªát v√† ti·∫øng Anh |

### **C√°ch ho·∫°t ƒë·ªông:**

```
Giai ƒëo·∫°n hu·∫•n luy·ªán (PhoBERT h·ªçc g√¨):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ VƒÉn b·∫£n ti·∫øng Vi·ªát ‚Üí M√£ nh√£n                    ‚îÇ
‚îÇ "C√¥ng ty ph·∫£i b·∫£o v·ªá d·ªØ li·ªáu" ‚Üí 5               ‚îÇ
‚îÇ "D·ªØ li·ªáu ph·∫£i ch√≠nh x√°c" ‚Üí 3                    ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ PhoBERT h·ªçc c√°c m·∫´u ng√¥n ng·ªØ ti·∫øng Vi·ªát!        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Giai ƒëo·∫°n d·ª± ƒëo√°n (Ng∆∞·ªùi d√πng th·∫•y g√¨):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ VƒÉn b·∫£n ti·∫øng Vi·ªát ‚Üí M√£ nh√£n ‚Üí Hi·ªÉn th·ªã         ‚îÇ
‚îÇ "D·ªØ li·ªáu ph·∫£i m√£ h√≥a" ‚Üí 5 ‚Üí "B·∫£o m·∫≠t" (VI)      ‚îÇ
‚îÇ                            ‚Üí "Integrity" (EN)    ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ B·∫°n ch·ªçn ng√¥n ng·ªØ hi·ªÉn th·ªã cho ng∆∞·ªùi d√πng!      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### **Th·ª±c h√†nh t·ªët:**
- ‚úÖ D√πng **m√£ s·ªë** trong code hu·∫•n luy·ªán (h∆∞·ªõng d·∫´n n√†y)
- ‚úÖ Hi·ªÉn th·ªã **ti·∫øng Vi·ªát** cho ng∆∞·ªùi d√πng Vi·ªát Nam (VeriPortal)
- ‚úÖ Hi·ªÉn th·ªã **ti·∫øng Anh** cho b√°o c√°o/API qu·ªëc t·∫ø
- ‚úÖ D·ªØ li·ªáu hu·∫•n luy·ªán **lu√¥n b·∫±ng ti·∫øng Vi·ªát** (PhoBERT h·ªçc ti·∫øng Vi·ªát!)

---

## **B∆∞·ªõc 2: C√†i ƒê·∫∑t C√°c G√≥i C·∫ßn Thi·∫øt**

### **2.1 T·∫°o M√¥i Tr∆∞·ªùng ·∫¢o**

```bash
# T·∫°o m√¥i tr∆∞·ªùng ·∫£o
python -m venv veriaidpo-env

# K√≠ch ho·∫°t (Windows)
veriaidpo-env\Scripts\activate

# K√≠ch ho·∫°t (Linux/Mac)
source veriaidpo-env/bin/activate
```

### **2.2 C√†i ƒê·∫∑t C√°c G√≥i**

```bash
# C√†i ƒë·∫∑t PyTorch (phi√™n b·∫£n CPU cho ki·ªÉm tra local)
pip install torch torchvision torchaudio

# C√†i ƒë·∫∑t Hugging Face Transformers
pip install transformers==4.35.0

# C√†i ƒë·∫∑t c√¥ng c·ª• hu·∫•n luy·ªán
pip install datasets==2.14.0
pip install accelerate==0.24.0
pip install scikit-learn==1.3.0

# C√†i ƒë·∫∑t VnCoreNLP (cho x·ª≠ l√Ω ti·∫øng Vi·ªát)
pip install vncorenlp==1.0.3

# T·∫£i VnCoreNLP JAR (m·ªôt l·∫ßn duy nh·∫•t)
# Windows PowerShell:
Invoke-WebRequest -Uri "https://github.com/vncorenlp/VnCoreNLP/raw/master/VnCoreNLP-1.2.jar" -OutFile "VnCoreNLP-1.2.jar"
```

### **2.3 Ki·ªÉm Tra C√†i ƒê·∫∑t**

```python
# kiem_tra_cai_dat.py
import torch
import transformers
from transformers import AutoTokenizer, AutoModelForSequenceClassification

print(f"Phi√™n b·∫£n PyTorch: {torch.__version__}")
print(f"Phi√™n b·∫£n Transformers: {transformers.__version__}")
print(f"CUDA kh·∫£ d·ª•ng: {torch.cuda.is_available()}")

# Ki·ªÉm tra t·∫£i PhoBERT
tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")
print("‚úÖ ƒê√£ t·∫£i th√†nh c√¥ng tokenizer PhoBERT")
```

---

## **B∆∞·ªõc 3: Ti·ªÅn X·ª≠ L√Ω D·ªØ Li·ªáu v·ªõi VnCoreNLP**

### **3.1 T·∫°i Sao C·∫ßn Ti·ªÅn X·ª≠ L√Ω?**

T√°ch t·ª´ ti·∫øng Vi·ªát c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c c·ªßa PhoBERT:
- **Kh√¥ng t√°ch t·ª´**: "C√¥ng ty ph·∫£i b·∫£o v·ªá d·ªØ li·ªáu" ‚Üí PhoBERT th·∫•y c√°c √¢m ti·∫øt
- **C√≥ VnCoreNLP**: "C√¥ng_ty ph·∫£i b·∫£o_v·ªá d·ªØ_li·ªáu" ‚Üí PhoBERT th·∫•y c√°c t·ª´

**C·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c**: +5-10%

**ƒê·∫∑c bi·ªát quan tr·ªçng cho ti·∫øng Vi·ªát** v√¨:
- Ti·∫øng Vi·ªát l√† ng√¥n ng·ªØ ƒë∆°n l·∫≠p (t·ª´ gh√©p t·ª´ nhi·ªÅu √¢m ti·∫øt)
- VD: "b·∫£o v·ªá" (2 √¢m ti·∫øt) = 1 t·ª´
- VnCoreNLP gi√∫p PhoBERT hi·ªÉu ranh gi·ªõi t·ª´ ƒë√∫ng

### **3.2 Script Ti·ªÅn X·ª≠ L√Ω**

```python
# tien_xu_ly_du_lieu.py
"""
Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu PDPL Vi·ªát Nam v·ªõi VnCoreNLP
"""

from vncorenlp import VnCoreNLP
import json

# Kh·ªüi t·∫°o VnCoreNLP
print("üîÑ ƒêang kh·ªüi t·∫°o VnCoreNLP...")
annotator = VnCoreNLP("./VnCoreNLP-1.2.jar", annotators="wseg", max_heap_size='-Xmx2g')
print("‚úÖ ƒê√£ kh·ªüi t·∫°o VnCoreNLP")

def tach_tu_tieng_viet(van_ban):
    """T√°ch t·ª´ vƒÉn b·∫£n ti·∫øng Vi·ªát v·ªõi VnCoreNLP"""
    tach = annotator.tokenize(van_ban)
    # N·ªëi c√°c t·ª´ b·∫±ng d·∫•u g·∫°ch d∆∞·ªõi
    xu_ly = ' '.join(['_'.join(cau) for cau in tach])
    return xu_ly

def xu_ly_tep(tep_dau_vao, tep_dau_ra):
    """X·ª≠ l√Ω t·ªáp JSONL"""
    so_dong = 0
    with open(tep_dau_vao, 'r', encoding='utf-8') as f_in:
        with open(tep_dau_ra, 'w', encoding='utf-8') as f_out:
            for dong in f_in:
                du_lieu = json.loads(dong)
                # T√°ch t·ª´ vƒÉn b·∫£n ti·∫øng Vi·ªát
                du_lieu['text'] = tach_tu_tieng_viet(du_lieu['text'])
                # Ghi v√†o t·ªáp ƒë·∫ßu ra
                f_out.write(json.dumps(du_lieu, ensure_ascii=False) + '\n')
                so_dong += 1
    print(f"‚úÖ ƒê√£ x·ª≠ l√Ω {so_dong} d√≤ng: {tep_dau_vao} ‚Üí {tep_dau_ra}")

# X·ª≠ l√Ω t·∫•t c·∫£ c√°c t·ªáp
print("\nüìä B·∫Øt ƒë·∫ßu ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu...")
xu_ly_tep('du_lieu/huan_luyen.jsonl', 'du_lieu/huan_luyen_da_xu_ly.jsonl')
xu_ly_tep('du_lieu/kiem_tra.jsonl', 'du_lieu/kiem_tra_da_xu_ly.jsonl')
xu_ly_tep('du_lieu/danh_gia.jsonl', 'du_lieu/danh_gia_da_xu_ly.jsonl')

annotator.close()
print("\n‚úÖ Ho√†n t·∫•t ti·ªÅn x·ª≠ l√Ω t·∫•t c·∫£ c√°c t·ªáp!")
```

**Ch·∫°y ti·ªÅn x·ª≠ l√Ω:**
```bash
python tien_xu_ly_du_lieu.py
```

**K·∫øt qu·∫£ mong ƒë·ª£i:**
```
üîÑ ƒêang kh·ªüi t·∫°o VnCoreNLP...
‚úÖ ƒê√£ kh·ªüi t·∫°o VnCoreNLP

üìä B·∫Øt ƒë·∫ßu ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu...
‚úÖ ƒê√£ x·ª≠ l√Ω 700 d√≤ng: du_lieu/huan_luyen.jsonl ‚Üí du_lieu/huan_luyen_da_xu_ly.jsonl
‚úÖ ƒê√£ x·ª≠ l√Ω 150 d√≤ng: du_lieu/kiem_tra.jsonl ‚Üí du_lieu/kiem_tra_da_xu_ly.jsonl
‚úÖ ƒê√£ x·ª≠ l√Ω 150 d√≤ng: du_lieu/danh_gia.jsonl ‚Üí du_lieu/danh_gia_da_xu_ly.jsonl

‚úÖ Ho√†n t·∫•t ti·ªÅn x·ª≠ l√Ω t·∫•t c·∫£ c√°c t·ªáp!
```

---

## **B∆∞·ªõc 4: T·∫£i v√† Chu·∫©n B·ªã D·ªØ Li·ªáu**

### **4.1 T·∫°o B·ªô T·∫£i D·ªØ Li·ªáu**

```python
# tai_du_lieu.py
"""
T·∫£i d·ªØ li·ªáu PDPL Vi·ªát Nam cho hu·∫•n luy·ªán PhoBERT
"""

from datasets import load_dataset
from transformers import AutoTokenizer

# T·∫£i tokenizer PhoBERT
print("üì• ƒêang t·∫£i tokenizer PhoBERT...")
tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")
print("‚úÖ ƒê√£ t·∫£i tokenizer PhoBERT")

# T·∫£i d·ªØ li·ªáu t·ª´ c√°c t·ªáp JSONL
print("\nüìÇ ƒêang t·∫£i d·ªØ li·ªáu...")
bo_du_lieu = load_dataset('json', data_files={
    'train': 'du_lieu/huan_luyen_da_xu_ly.jsonl',
    'validation': 'du_lieu/kiem_tra_da_xu_ly.jsonl',
    'test': 'du_lieu/danh_gia_da_xu_ly.jsonl'
})

print(f"‚úÖ ƒê√£ t·∫£i d·ªØ li·ªáu:")
print(f"  Hu·∫•n luy·ªán: {len(bo_du_lieu['train'])} m·∫´u")
print(f"  Ki·ªÉm tra: {len(bo_du_lieu['validation'])} m·∫´u")
print(f"  ƒê√°nh gi√°: {len(bo_du_lieu['test'])} m·∫´u")

# H√†m tokenize
def ham_tokenize(cac_mau):
    """Tokenize vƒÉn b·∫£n ti·∫øng Vi·ªát cho PhoBERT"""
    return tokenizer(
        cac_mau['text'],
        padding='max_length',
        truncation=True,
        max_length=256  # ƒê·ªô d√†i chu·ªói t·ªëi ƒëa
    )

# Tokenize t·∫•t c·∫£ d·ªØ li·ªáu
print("\nüîÑ ƒêang tokenize d·ªØ li·ªáu...")
bo_du_lieu_tokenized = bo_du_lieu.map(ham_tokenize, batched=True)

# X√≥a c·ªôt vƒÉn b·∫£n g·ªëc (gi·ªØ input_ids, attention_mask ƒë√£ tokenize)
bo_du_lieu_tokenized = bo_du_lieu_tokenized.remove_columns(['text'])

# ƒê·ªïi t√™n 'label' th√†nh 'labels' (y√™u c·∫ßu b·ªüi Trainer)
bo_du_lieu_tokenized = bo_du_lieu_tokenized.rename_column('label', 'labels')

print("‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c tokenize v√† s·∫µn s√†ng cho hu·∫•n luy·ªán")
```

---

## **B∆∞·ªõc 5: Hu·∫•n Luy·ªán PhoBERT**

### **5.1 Script Hu·∫•n Luy·ªán (Ti·∫øng Vi·ªát)**

```python
# huan_luyen_phobert.py
"""
Hu·∫•n luy·ªán PhoBERT tr√™n d·ªØ li·ªáu tu√¢n th·ªß PDPL Vi·ªát Nam
"""

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
from datasets import load_dataset
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# ƒê·∫∑t seed ng·∫´u nhi√™n ƒë·ªÉ t√°i t·∫°o k·∫øt qu·∫£
torch.manual_seed(42)

print("="*60)
print("üáªüá≥ HU·∫§N LUY·ªÜN PHOBERT CHO PDPL VI·ªÜT NAM üáªüá≥")
print("="*60)

# T·∫£i tokenizer
print("\nüì• ƒêang t·∫£i tokenizer PhoBERT...")
tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")
print("‚úÖ ƒê√£ t·∫£i tokenizer")

# T·∫£i d·ªØ li·ªáu
print("\nüìÇ ƒêang t·∫£i d·ªØ li·ªáu...")
bo_du_lieu = load_dataset('json', data_files={
    'train': 'du_lieu/huan_luyen_da_xu_ly.jsonl',
    'validation': 'du_lieu/kiem_tra_da_xu_ly.jsonl',
    'test': 'du_lieu/danh_gia_da_xu_ly.jsonl'
})

print(f"‚úÖ ƒê√£ t·∫£i d·ªØ li·ªáu:")
print(f"  üìö Hu·∫•n luy·ªán: {len(bo_du_lieu['train'])} m·∫´u")
print(f"  üîç Ki·ªÉm tra: {len(bo_du_lieu['validation'])} m·∫´u")
print(f"  üìä ƒê√°nh gi√°: {len(bo_du_lieu['test'])} m·∫´u")

# H√†m tokenize
def ham_tokenize(cac_mau):
    return tokenizer(
        cac_mau['text'],
        padding='max_length',
        truncation=True,
        max_length=256
    )

# Tokenize d·ªØ li·ªáu
print("\nüîÑ ƒêang tokenize d·ªØ li·ªáu...")
bo_du_lieu_tokenized = bo_du_lieu.map(ham_tokenize, batched=True)
bo_du_lieu_tokenized = bo_du_lieu_tokenized.remove_columns(['text'])
bo_du_lieu_tokenized = bo_du_lieu_tokenized.rename_column('label', 'labels')
print("‚úÖ Ho√†n t·∫•t tokenize")

# T·∫£i m√¥ h√¨nh PhoBERT (8 l·ªõp ƒë·∫ßu ra cho 8 danh m·ª•c PDPL)
print("\nüì• ƒêang t·∫£i m√¥ h√¨nh PhoBERT...")
mo_hinh = AutoModelForSequenceClassification.from_pretrained(
    "vinai/phobert-base",
    num_labels=8  # 8 nguy√™n t·∫Øc tu√¢n th·ªß PDPL
)
print("‚úÖ ƒê√£ t·∫£i m√¥ h√¨nh PhoBERT")

# Data collator
du_lieu_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# ƒê·ªãnh nghƒ©a metrics
def tinh_metrics(eval_pred):
    """T√≠nh accuracy, precision, recall, F1"""
    du_doan, nhan = eval_pred
    du_doan = np.argmax(du_doan, axis=1)
    
    do_chinh_xac = accuracy_score(nhan, du_doan)
    do_chinh_xac_chi_tiet, recall, f1, _ = precision_recall_fscore_support(
        nhan, du_doan, average='weighted'
    )
    
    return {
        'do_chinh_xac': do_chinh_xac,
        'precision': do_chinh_xac_chi_tiet,
        'recall': recall,
        'f1': f1
    }

# Tham s·ªë hu·∫•n luy·ªán (t·ªëi ∆∞u cho PC Vi·ªát Nam)
tham_so_huan_luyen = TrainingArguments(
    output_dir='./phobert-pdpl-viet-nam',
    
    # Tham s·ªë hu·∫•n luy·ªán
    num_train_epochs=5,              # S·ªë epoch hu·∫•n luy·ªán
    per_device_train_batch_size=16,  # K√≠ch th∆∞·ªõc batch hu·∫•n luy·ªán
    per_device_eval_batch_size=32,   # K√≠ch th∆∞·ªõc batch ƒë√°nh gi√°
    learning_rate=2e-5,              # T·ªëc ƒë·ªô h·ªçc
    weight_decay=0.01,               # Weight decay (L2 regularization)
    warmup_steps=500,                # B∆∞·ªõc kh·ªüi ƒë·ªông cho learning rate
    
    # ƒê√°nh gi√° & l∆∞u
    evaluation_strategy='epoch',     # ƒê√°nh gi√° m·ªói epoch
    save_strategy='epoch',           # L∆∞u checkpoint m·ªói epoch
    load_best_model_at_end=True,    # T·∫£i m√¥ h√¨nh t·ªët nh·∫•t sau hu·∫•n luy·ªán
    metric_for_best_model='do_chinh_xac',  # D√πng ƒë·ªô ch√≠nh x√°c ƒë·ªÉ ch·ªçn m√¥ h√¨nh t·ªët nh·∫•t
    
    # Logging
    logging_dir='./logs',
    logging_steps=100,
    
    # Hi·ªáu su·∫•t
    fp16=torch.cuda.is_available(),  # D√πng mixed precision n·∫øu c√≥ GPU
    dataloader_num_workers=0,        # S·ªë worker t·∫£i d·ªØ li·ªáu
)

# Kh·ªüi t·∫°o Trainer
print("\nüéØ ƒêang kh·ªüi t·∫°o Trainer...")
trainer = Trainer(
    model=mo_hinh,
    args=tham_so_huan_luyen,
    train_dataset=bo_du_lieu_tokenized['train'],
    eval_dataset=bo_du_lieu_tokenized['validation'],
    tokenizer=tokenizer,
    data_collator=du_lieu_collator,
    compute_metrics=tinh_metrics,
)
print("‚úÖ ƒê√£ kh·ªüi t·∫°o Trainer")

# Hu·∫•n luy·ªán m√¥ h√¨nh
print("\n" + "="*60)
print("üöÄ B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN PHOBERT...")
print("="*60 + "\n")

trainer.train()

# ƒê√°nh gi√° tr√™n t·∫≠p ki·ªÉm tra
print("\n" + "="*60)
print("üìä ƒê√ÅNH GI√Å TR√äN T·∫¨P KI·ªÇM TRA...")
print("="*60 + "\n")

ket_qua_test = trainer.evaluate(bo_du_lieu_tokenized['test'])
print(f"\n‚úÖ K·∫øt qu·∫£ ki·ªÉm tra:")
for ten_metric, gia_tri in ket_qua_test.items():
    print(f"  {ten_metric}: {gia_tri:.4f}")

# L∆∞u m√¥ h√¨nh cu·ªëi c√πng
print("\nüíæ ƒêang l∆∞u m√¥ h√¨nh...")
trainer.save_model('./phobert-pdpl-viet-nam-final')
tokenizer.save_pretrained('./phobert-pdpl-viet-nam-final')

print("\n" + "="*60)
print("‚úÖ HO√ÄN T·∫§T HU·∫§N LUY·ªÜN!")
print("üìÅ M√¥ h√¨nh ƒë√£ l∆∞u t·∫°i: ./phobert-pdpl-viet-nam-final")
print("="*60)
```

### **5.2 Ch·∫°y Hu·∫•n Luy·ªán**

```bash
# B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán (CPU - ch·∫≠m h∆°n)
python huan_luyen_phobert.py

# HO·∫∂C v·ªõi GPU (nhanh h∆°n) - Linux/Mac
CUDA_VISIBLE_DEVICES=0 python huan_luyen_phobert.py

# HO·∫∂C v·ªõi GPU (nhanh h∆°n) - Windows PowerShell
$env:CUDA_VISIBLE_DEVICES="0"; python huan_luyen_phobert.py
```

**Th·ªùi gian hu·∫•n luy·ªán d·ª± ki·∫øn** (PC c·ªßa b·∫°n - Intel Iris Xe):
- **CPU**: 2.5-4 gi·ªù (1000 m·∫´u, 5 epochs)
- **Google Colab GPU**: 15-30 ph√∫t (**Khuy·∫øn ngh·ªã!**)

---

## **üìä Th·ªùi Gian Hu·∫•n Luy·ªán D·ª± Ki·∫øn (PC Vi·ªát Nam)**

### **PC C·ªßa B·∫°n: Intel Iris Xe Graphics**

| Ph·∫ßn c·ª©ng | Th·ªùi gian m·ªói epoch | T·ªïng th·ªùi gian (5 epochs) |
|-----------|---------------------|---------------------------|
| **CPU (Intel)** | ~35-45 ph√∫t | **2.5-4 gi·ªù** |
| **ƒê·ªÅ xu·∫•t: Google Colab GPU** | ~3-4 ph√∫t | **15-30 ph√∫t** |

### **Chi·∫øn L∆∞·ª£c T·ªëi ∆Øu:**

1. ‚úÖ **Ki·ªÉm tra v·ªõi 100 m·∫´u ƒë·∫ßu ti√™n** (20 ph√∫t tr√™n PC)
   - ƒê·∫£m b·∫£o m·ªçi th·ª© ho·∫°t ƒë·ªông
   - Ki·ªÉm tra PC c√≥ ƒë·ªß kh·∫£ nƒÉng

2. ‚úÖ **N·∫øu ki·ªÉm tra th√†nh c√¥ng, ch·ªçn**:
   - **Ki√™n nh·∫´n**: Hu·∫•n luy·ªán qua ƒë√™m tr√™n PC (3-4 gi·ªù, mi·ªÖn ph√≠)
   - **Nhanh ch√≥ng**: D√πng Google Colab GPU (30 ph√∫t, mi·ªÖn ph√≠)

3. ‚úÖ **Cho s·∫£n xu·∫•t** (2000+ m·∫´u):
   - D√πng AWS SageMaker (xem `VeriAIDPO_ML_AWS_Training_Plan.md`)
   - Chi ph√≠: ~$5-10 cho hu·∫•n luy·ªán, 10-20 ph√∫t

---

## **üéØ M·∫πo ƒê·ªÉ K·∫øt Qu·∫£ T·ªët H∆°n**

### **Ch·∫•t L∆∞·ª£ng D·ªØ Li·ªáu**
- ‚úÖ **Nhi·ªÅu d·ªØ li·ªáu = ƒë·ªô ch√≠nh x√°c cao h∆°n** (m·ª•c ti√™u 2000+ m·∫´u)
- ‚úÖ **C√¢n b·∫±ng c√°c l·ªõp** (s·ªë m·∫´u b·∫±ng nhau cho m·ªói danh m·ª•c)
- ‚úÖ **V√≠ d·ª• th·ª±c t·∫ø** (vƒÉn b·∫£n tu√¢n th·ªß PDPL th·ª±c t·∫ø)
- ‚úÖ **ƒêa d·∫°ng v√πng mi·ªÅn** (B·∫Øc, Trung, Nam Vi·ªát Nam)

### **T·ªëi ∆Øu Hu·∫•n Luy·ªán**
- ‚úÖ **D√πng GPU** (nhanh h∆°n CPU 10-20 l·∫ßn)
- ‚úÖ **TƒÉng epochs** (5-10 epochs ƒë·ªÉ h·ªôi t·ª• t·ªët h∆°n)
- ‚úÖ **ƒêi·ªÅu ch·ªânh learning rate** (th·ª≠ 1e-5, 2e-5, 3e-5)
- ‚úÖ **D√πng VnCoreNLP preprocessing** (+5-10% ƒë·ªô ch√≠nh x√°c)

### **C·∫£i Thi·ªán M√¥ H√¨nh**
- ‚úÖ **Th·ª≠ PhoBERT-large** (ƒë·ªô ch√≠nh x√°c cao h∆°n, ch·∫≠m h∆°n)
- ‚úÖ **Ensemble models** (k·∫øt h·ª£p nhi·ªÅu m√¥ h√¨nh)
- ‚úÖ **Data augmentation** (paraphrase, back-translation)
- ‚úÖ **Active learning** (hu·∫•n luy·ªán l·∫°i v·ªõi c√°c m·∫´u sai)

---

## **‚úÖ C√°c B∆∞·ªõc Ti·∫øp Theo**

Sau khi hu·∫•n luy·ªán m√¥ h√¨nh:

1. ‚úÖ **Tri·ªÉn khai l√™n AWS** (xem `VeriAIDPO_ML_AWS_Training_Plan.md`)
2. ‚úÖ **T√≠ch h·ª£p v·ªõi VeriPortal** (th√™m v√†o compliance wizards)
3. ‚úÖ **H·ªçc li√™n t·ª•c** (hu·∫•n luy·ªán l·∫°i h√†ng th√°ng v·ªõi d·ªØ li·ªáu m·ªõi)
4. ‚úÖ **A/B testing** (so s√°nh v·ªõi baseline)
5. ‚úÖ **Ch·ª©ng nh·∫≠n ISO 42001** (ghi l·∫°i quy tr√¨nh hu·∫•n luy·ªán)

---

**B·∫°n ƒë√£ s·∫µn s√†ng ƒë·ªÉ hu·∫•n luy·ªán PhoBERT tr√™n d·ªØ li·ªáu PDPL Vi·ªát Nam!** üöÄüáªüá≥

---

*Phi√™n b·∫£n t√†i li·ªáu: 1.0*
*C·∫≠p nh·∫≠t l·∫ßn cu·ªëi: 5 th√°ng 10, 2025*
*Ng∆∞·ªùi s·ªü h·ªØu: Nh√≥m AI/ML VeriSyntra*
