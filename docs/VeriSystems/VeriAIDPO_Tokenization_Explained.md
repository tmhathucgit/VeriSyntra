# VeriAIDPO - Tokenization Explained
## What Happens When You "Tokenize Datasets"?

### **Executive Summary**

Tokenization converts Vietnamese text into numbers (tokens) that PhoBERT can process. This document explains the tokenization process step-by-step with visual examples.

---

## **ğŸ¯ Simple Definition**

**Tokenization** = Converting text into numbers that AI can understand

```
Vietnamese Text  â†’  [Tokenization]  â†’  Numbers  â†’  [PhoBERT]  â†’  Prediction
"Báº£o vá»‡ dá»¯ liá»‡u"                     [1,2,3,4,5]
```

---

## **ğŸ“– Real Example: Step-by-Step**

### **Input: Vietnamese PDPL Text**

```json
{
  "text": "CÃ´ng ty pháº£i báº£o vá»‡ dá»¯ liá»‡u cÃ¡ nhÃ¢n",
  "label": 5
}
```

### **Step 1: Load PhoBERT Tokenizer**

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")
```

**What is a tokenizer?**
- A dictionary that maps Vietnamese words/syllables to numbers
- PhoBERT's tokenizer has ~64,000 Vietnamese words/tokens
- Example: "dá»¯_liá»‡u" â†’ Token ID 15432

### **Step 2: Tokenize Single Text**

```python
text = "CÃ´ng ty pháº£i báº£o vá»‡ dá»¯ liá»‡u cÃ¡ nhÃ¢n"

# Tokenize
result = tokenizer(text)

print(result)
```

**Output:**
```python
{
  'input_ids': [2, 8901, 1234, 5432, 9876, 3210, 15432, 7654, 3456, 3],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

**Explanation:**
- `input_ids`: Vietnamese words converted to numbers
  - `2` = `<s>` (sentence start token)
  - `8901` = "CÃ´ng"
  - `1234` = "ty"
  - `5432` = "pháº£i"
  - ... (and so on)
  - `3` = `</s>` (sentence end token)

- `attention_mask`: Tells PhoBERT which tokens are real (1) vs padding (0)
  - All `1` means all tokens are real (no padding yet)

---

## **ğŸ”¢ Visual Breakdown: Token IDs**

### **Before Tokenization:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Text: "CÃ´ng ty pháº£i báº£o vá»‡ dá»¯ liá»‡u cÃ¡ nhÃ¢n"   â”‚
â”‚                                                â”‚
â”‚ PhoBERT cannot read this! âŒ                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **After Tokenization:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Token IDs: [2, 8901, 1234, 5432, 9876, 3210,  â”‚
â”‚             15432, 7654, 3456, 3]             â”‚
â”‚                                                â”‚
â”‚ PhoBERT can read this! âœ…                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Token Mapping Table:**

| Position | Token ID | Vietnamese Word | English Meaning |
|----------|----------|-----------------|-----------------|
| 0 | 2 | `<s>` | Sentence start |
| 1 | 8901 | CÃ´ng | Company |
| 2 | 1234 | ty | (part of company) |
| 3 | 5432 | pháº£i | must |
| 4 | 9876 | báº£o | protect |
| 5 | 3210 | vá»‡ | (part of protect) |
| 6 | 15432 | dá»¯_liá»‡u | data |
| 7 | 7654 | cÃ¡ | personal |
| 8 | 3456 | nhÃ¢n | (part of personal) |
| 9 | 3 | `</s>` | Sentence end |

---

## **ğŸ“¦ Batch Tokenization (Multiple Texts)**

### **Input: Multiple Training Examples**

```python
texts = [
    "CÃ´ng ty pháº£i báº£o vá»‡ dá»¯ liá»‡u",
    "Dá»¯ liá»‡u pháº£i chÃ­nh xÃ¡c",
    "KhÃ´ng lÆ°u trá»¯ dá»¯ liá»‡u quÃ¡ lÃ¢u"
]

# Tokenize all at once
result = tokenizer(
    texts,
    padding=True,        # Make all same length
    truncation=True,     # Cut if too long
    max_length=20        # Maximum 20 tokens
)

print(result['input_ids'])
```

**Output:**
```python
[
  [2, 8901, 1234, 5432, 9876, 3210, 15432, 3, 1, 1, 1, 1],  # Text 1 (padded)
  [2, 15432, 5432, 6789, 4321, 3, 1, 1, 1, 1, 1, 1],        # Text 2 (padded)
  [2, 9999, 8888, 7777, 15432, 6666, 5555, 3, 1, 1, 1, 1]   # Text 3 (padded)
]
```

**Notice:**
- All sequences are now **12 tokens long** (same length)
- Short sequences have `1` (padding token) at the end
- All start with `2` (`<s>`) and end with `3` (`</s>`)

---

## **ğŸ¨ Padding & Truncation**

### **Problem: Different Length Sequences**

```python
Text 1: "CÃ´ng ty pháº£i báº£o vá»‡ dá»¯ liá»‡u"                    â†’ 8 tokens
Text 2: "Dá»¯ liá»‡u pháº£i chÃ­nh xÃ¡c"                         â†’ 6 tokens
Text 3: "CÃ´ng ty pháº£i tuÃ¢n thá»§ Nghá»‹ Ä‘á»‹nh 13/2023..."    â†’ 25 tokens
```

PhoBERT requires **all inputs same length** (like a table with equal rows).

### **Solution 1: Padding (Add Fake Tokens)**

```python
tokenizer(texts, padding='max_length', max_length=12)
```

**Result:**
```python
Text 1: [2, 8901, 1234, 5432, 9876, 3210, 15432, 3, 1, 1, 1, 1]  â† Added 4 padding
Text 2: [2, 15432, 5432, 6789, 4321, 3, 1, 1, 1, 1, 1, 1]        â† Added 6 padding
Text 3: [2, 8901, 1234, 5432, 9999, 8888, 7777, 6666, 5555, 4444, 3333, 3]  â† Truncated!
                                                                     â†‘ Cut here
```

### **Solution 2: Attention Mask (Tell PhoBERT Which Tokens Are Real)**

```python
{
  'input_ids':      [2, 8901, 1234, 3, 1, 1, 1],
  'attention_mask': [1, 1,    1,    1, 0, 0, 0]
                                    â†‘ Real  â†‘ Padding (ignore)
}
```

- `1` = Real token (PhoBERT pays attention)
- `0` = Padding token (PhoBERT ignores)

---

## **ğŸ”§ Tokenization in Training Code**

### **From the Training Guide (Step 4)**

```python
# load_dataset.py
from datasets import load_dataset
from transformers import AutoTokenizer

# Load PhoBERT tokenizer
tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")

# Load your PDPL dataset
dataset = load_dataset('json', data_files={
    'train': 'data/train_preprocessed.jsonl',
    'validation': 'data/val_preprocessed.jsonl',
    'test': 'data/test_preprocessed.jsonl'
})

# Define tokenization function
def tokenize_function(examples):
    """Tokenize Vietnamese text for PhoBERT"""
    return tokenizer(
        examples['text'],        # Vietnamese text
        padding='max_length',    # Pad to max_length
        truncation=True,         # Truncate if too long
        max_length=256           # Maximum 256 tokens
    )

# Apply tokenization to entire dataset
tokenized_dataset = dataset.map(tokenize_function, batched=True)
```

### **What Happens:**

**Before tokenization:**
```python
dataset['train'][0]
# Output:
{
  'text': 'CÃ´ng_ty pháº£i báº£o_vá»‡ dá»¯_liá»‡u cÃ¡_nhÃ¢n',
  'label': 5
}
```

**After tokenization:**
```python
tokenized_dataset['train'][0]
# Output:
{
  'input_ids': [2, 8901, 1234, 5432, 9876, 3210, 15432, 7654, 3456, 3, 1, 1, ...],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...],
  'labels': 5  # (renamed from 'label')
}
```

**Key changes:**
- âœ… `text` is now `input_ids` (numbers)
- âœ… Added `attention_mask` (which tokens are real)
- âœ… `label` renamed to `labels` (required by Trainer)

---

## **ğŸ§  Why Tokenization Matters**

### **Without Tokenization:**
```python
# âŒ PhoBERT cannot process raw text
model(text="CÃ´ng ty pháº£i báº£o vá»‡ dá»¯ liá»‡u")  # Error!
```

### **With Tokenization:**
```python
# âœ… PhoBERT processes token IDs
inputs = tokenizer("CÃ´ng ty pháº£i báº£o vá»‡ dá»¯ liá»‡u", return_tensors="pt")
model(**inputs)  # Works! Returns predictions
```

---

## **ğŸ“Š Tokenization Parameters Explained**

### **Common Parameters:**

| Parameter | What It Does | Example Value |
|-----------|--------------|---------------|
| `padding` | Add fake tokens to make all sequences same length | `'max_length'` |
| `truncation` | Cut sequences if they're too long | `True` |
| `max_length` | Maximum number of tokens | `256` |
| `return_tensors` | Return PyTorch tensors (for model input) | `'pt'` |

### **Example with All Parameters:**

```python
result = tokenizer(
    "CÃ´ng ty pháº£i báº£o vá»‡ dá»¯ liá»‡u cÃ¡ nhÃ¢n",
    padding='max_length',     # Pad to 256 tokens
    truncation=True,          # Cut if > 256 tokens
    max_length=256,           # Max 256 tokens
    return_tensors='pt'       # Return PyTorch tensor
)

print(result.keys())
# Output: dict_keys(['input_ids', 'attention_mask'])

print(result['input_ids'].shape)
# Output: torch.Size([1, 256])  â† 1 sequence, 256 tokens
```

---

## **ğŸ¯ Real Training Example**

### **Complete Tokenization Workflow:**

```python
# Step 1: Load data
dataset = load_dataset('json', data_files='data/train.jsonl')

# Example data:
# {"text": "CÃ´ng ty pháº£i báº£o vá»‡ dá»¯ liá»‡u", "label": 5}
# {"text": "Dá»¯ liá»‡u pháº£i chÃ­nh xÃ¡c", "label": 3}

# Step 2: Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")

# Step 3: Define tokenization function
def tokenize_function(examples):
    return tokenizer(
        examples['text'],
        padding='max_length',
        truncation=True,
        max_length=256
    )

# Step 4: Tokenize entire dataset
tokenized_dataset = dataset.map(tokenize_function, batched=True)

# Step 5: Prepare for training
tokenized_dataset = tokenized_dataset.remove_columns(['text'])  # Remove original text
tokenized_dataset = tokenized_dataset.rename_column('label', 'labels')  # Rename label

# Step 6: Now ready for PhoBERT training!
# PhoBERT receives: input_ids + attention_mask â†’ Predicts: label
```

---

## **â“ Common Questions**

### **Q1: Why convert text to numbers?**
**A:** Neural networks (like PhoBERT) can only process numbers, not text. Tokenization is the bridge.

### **Q2: What are Token IDs?**
**A:** Unique numbers assigned to each Vietnamese word/syllable in PhoBERT's vocabulary (64,000 tokens total).

### **Q3: What is padding?**
**A:** Adding fake tokens (ID=1) to make all sequences the same length (required for batch processing).

### **Q4: What is attention_mask?**
**A:** A list of 1s and 0s telling PhoBERT which tokens are real (1) and which are padding (0).

### **Q5: Why max_length=256?**
**A:** PhoBERT can process up to 512 tokens, but 256 is enough for most PDPL compliance text and trains faster.

### **Q6: Can I see the original words from Token IDs?**
**A:** Yes! Use `tokenizer.decode()`:

```python
token_ids = [2, 8901, 1234, 5432, 3]
text = tokenizer.decode(token_ids)
print(text)
# Output: "CÃ´ng ty pháº£i"
```

---

## **ğŸ” Visual Summary**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TOKENIZATION PROCESS                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Raw Data (JSONL file):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ {"text": "CÃ´ng ty pháº£i báº£o vá»‡ dá»¯ liá»‡u", "label": 5}        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                    [TOKENIZATION]
                            â†“
Tokenized Data (Numbers):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ {                                                           â”‚
â”‚   "input_ids": [2, 8901, 1234, 5432, 9876, 3210, ...],    â”‚
â”‚   "attention_mask": [1, 1, 1, 1, 1, 1, ...],              â”‚
â”‚   "labels": 5                                              â”‚
â”‚ }                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                      [PHOBERT MODEL]
                            â†“
Prediction:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Category: "Integrity and confidentiality"                  â”‚
â”‚ Confidence: 94.32%                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## **âœ… Key Takeaways**

1. âœ… **Tokenization = Text â†’ Numbers** (so AI can read it)
2. âœ… **Token IDs** = Unique numbers for Vietnamese words
3. âœ… **Padding** = Make all sequences same length
4. âœ… **Attention Mask** = Tell AI which tokens are real
5. âœ… **Max Length 256** = Good for PDPL text (not too long)
6. âœ… **Happens automatically** = Just call `tokenizer(text)`

---

## **ğŸš€ Next Steps**

After understanding tokenization:
1. âœ… Read `VeriAIDPO_PhoBERT_Training_Guide.md` (Step 4-5)
2. âœ… Run tokenization on your PDPL dataset
3. âœ… Train PhoBERT with tokenized data
4. âœ… PhoBERT learns to predict compliance categories!

---

**Tokenization is the foundation of PhoBERT training!** ğŸ¯

Without it, PhoBERT cannot read Vietnamese text.
With it, PhoBERT becomes a Vietnamese PDPL compliance expert! ğŸ‡»ğŸ‡³

---

*Document Version: 1.0*
*Last Updated: October 5, 2025*
*Owner: VeriSyntra AI/ML Team*
