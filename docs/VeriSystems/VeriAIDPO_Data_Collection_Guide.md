# VeriAIDPO - Vietnamese PDPL Data Collection Guide
## Hybrid Approach Strategy for Production-Quality Dataset

> **üáªüá≥ Vietnamese-First Data Collection**: This guide provides a comprehensive strategy for collecting 5,000+ Vietnamese PDPL compliance examples using a hybrid approach combining official sources, synthetic generation, and crowdsourcing.

---

## **Executive Summary**

**üéØ Goal**: Collect 4,500-5,000 Vietnamese PDPL examples for PhoBERT training  
**‚è±Ô∏è Timeline**: 3-6 weeks (MVP) or 4-8 weeks (Production)  
**üí∞ Estimated Cost**: **$0 (MVP)** or **$1,500 (Production)**  
**üéØ Target Accuracy**: **90-93% (MVP)** or **95-97% (Production)**

---

### **üöÄ MVP Path (Recommended for Prototype)**

**Perfect for**: Investor demo, proof of concept, initial testing, $0 budget

| Source | Examples | % of Total | Quality | Cost | Timeline |
|--------|----------|------------|---------|------|----------|
| **Official Documents** | 1,500 | 33% | High | Free | 2 weeks |
| **Media Articles** | 1,000 | 22% | Medium | Free | 1 week |
| **Business Documents** | 1,000 | 22% | Medium-High | Free | 1 week |
| **Synthetic Data** | 1,000 | 22% | Controlled | Free | 3 days |
| **MVP TOTAL** | **4,500** | **100%** | **Mixed** | **$0** | **3-4 weeks** |

**Expected Accuracy**: 90-93% across Vietnamese regions  
**When to use**: Pre-funding, demo stage, rapid prototyping

---

### **üéØ Production Path (Post-Funding)**

**Perfect for**: Enterprise customers, production launch, seed-funded startups

| Source | Examples | % of Total | Quality | Cost | Timeline |
|--------|----------|------------|---------|------|----------|
| **Official Documents** | 1,500 | 30% | High | Free | 2 weeks |
| **Media Articles** | 1,000 | 20% | Medium | Free | 1 week |
| **Business Documents** | 1,000 | 20% | Medium-High | Free | 1 week |
| **Synthetic Data** | 1,000 | 20% | Controlled | Free | 3 days |
| **Crowdsourced** ‚≠ê | 500 | 10% | Authentic | $1,500 | 2 weeks |
| **PRODUCTION TOTAL** | **5,000** | **100%** | **Premium** | **$1,500** | **4-6 weeks** |

**Expected Accuracy**: 95-97% across Vietnamese regions  
**When to use**: Post seed funding, enterprise sales, production deployment

**Regional Balance (Both Paths)**: 33% B·∫Øc, 33% Trung, 34% Nam

---

## **Part 1: Official Vietnamese Documents (1,500 examples)**

### **1.1 Primary Legal Sources**

```python
# collect_official_documents.py
"""
Collect Vietnamese PDPL data from official government sources
High quality, legally accurate examples
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime

# Official Vietnamese Legal Databases
LEGAL_SOURCES = {
    'thuvienphapluat': 'https://thuvienphapluat.vn/',
    'moj_vietnam': 'https://moj.gov.vn/',
    'mps_vietnam': 'https://bocongan.gov.vn/',
    'national_assembly': 'https://quochoi.vn/'
}

# PDPL 2025 Related Documents
PDPL_DOCUMENTS = {
    'law_91_2025': {
        'name': 'Lu·∫≠t B·∫£o v·ªá D·ªØ li·ªáu C√° nh√¢n s·ªë 91/2025/QH15',
        'url': 'https://thuvienphapluat.vn/van-ban/Cong-nghe-thong-tin/...',
        'type': 'primary_law',
        'priority': 'high'
    },
    'decree_13_2023': {
        'name': 'Ngh·ªã ƒë·ªãnh 13/2023/Nƒê-CP',
        'url': 'https://thuvienphapluat.vn/van-ban/Cong-nghe-thong-tin/...',
        'type': 'implementation_decree',
        'priority': 'high'
    },
    'mps_circulars': {
        'name': 'Th√¥ng t∆∞ B·ªô C√¥ng an v·ªÅ PDPL',
        'url': 'https://bocongan.gov.vn/...',
        'type': 'circular',
        'priority': 'medium'
    }
}

def extract_pdpl_sections(document_url, document_type):
    """
    Extract PDPL-related sections from Vietnamese legal documents
    Returns: List of Vietnamese text snippets with metadata
    """
    
    examples = []
    
    try:
        response = requests.get(document_url, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Extract Vietnamese paragraphs
        paragraphs = soup.find_all(['p', 'div'], class_=['article', 'content'])
        
        for para in paragraphs:
            text = para.get_text(strip=True)
            
            # Filter Vietnamese PDPL-related content (min 50 chars)
            if len(text) >= 50 and any(keyword in text.lower() for keyword in [
                'd·ªØ li·ªáu c√° nh√¢n', 'b·∫£o v·ªá', 'quy·ªÅn', 'ch·ªß th·ªÉ', 
                'x·ª≠ l√Ω', 'thu th·∫≠p', 'l∆∞u tr·ªØ', 'b·∫£o m·∫≠t'
            ]):
                examples.append({
                    'text': text,
                    'source': document_url,
                    'type': document_type,
                    'date_collected': datetime.now().isoformat(),
                    'quality': 'high',
                    'origin': 'official'
                })
    
    except Exception as e:
        print(f"Error extracting from {document_url}: {e}")
    
    return examples

# Collect from all official sources
all_official_examples = []

for doc_id, doc_info in PDPL_DOCUMENTS.items():
    print(f"üìÑ Collecting from: {doc_info['name']}")
    examples = extract_pdpl_sections(doc_info['url'], doc_info['type'])
    all_official_examples.extend(examples)
    print(f"   ‚úÖ Collected {len(examples)} examples")

print(f"\nüéØ Total official examples: {len(all_official_examples)}")
```

**Expected Output**: 1,500 high-quality Vietnamese examples from official sources

---

## **Part 2: Vietnamese Media Articles (1,000 examples)**

```python
# collect_media_articles.py
"""
Collect Vietnamese PDPL articles from news media
Medium quality, current context
"""

VIETNAMESE_MEDIA = {
    'vnexpress': 'https://vnexpress.net/phap-luat',
    'tuoitre': 'https://tuoitre.vn/phap-luat.htm',
    'thanhnien': 'https://thanhnien.vn/phap-luat/',
    'vietnamnet': 'https://vietnamnet.vn/phap-luat',
    'dantri': 'https://dantri.com.vn/phap-luat.htm'
}

SEARCH_KEYWORDS = [
    'b·∫£o v·ªá d·ªØ li·ªáu c√° nh√¢n',
    'PDPL 2025',
    'Ngh·ªã ƒë·ªãnh 13/2023',
    'quy·ªÅn ri√™ng t∆∞ d·ªØ li·ªáu',
    'an to√†n th√¥ng tin c√° nh√¢n',
    'tu√¢n th·ªß b·∫£o v·ªá d·ªØ li·ªáu'
]

def search_media_pdpl_articles(media_url, keywords):
    """Search Vietnamese media for PDPL articles"""
    
    articles = []
    
    for keyword in keywords:
        search_url = f"{media_url}/tim-kiem?q={keyword}"
        
        try:
            response = requests.get(search_url)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract article snippets (Vietnamese)
            article_divs = soup.find_all('div', class_=['article-item', 'news-item'])
            
            for article in article_divs[:20]:  # Max 20 per keyword
                title = article.find(['h3', 'h2', 'a'])
                description = article.find('p', class_='description')
                
                if title and description:
                    combined_text = f"{title.get_text(strip=True)}. {description.get_text(strip=True)}"
                    
                    articles.append({
                        'text': combined_text,
                        'source': media_url,
                        'keyword': keyword,
                        'type': 'media_article',
                        'quality': 'medium'
                    })
        
        except Exception as e:
            print(f"Error searching {media_url}: {e}")
    
    return articles

# Collect from all media sources
media_examples = []

for media_name, media_url in VIETNAMESE_MEDIA.items():
    print(f"üì∞ Collecting from: {media_name}")
    articles = search_media_pdpl_articles(media_url, SEARCH_KEYWORDS)
    media_examples.extend(articles)
    print(f"   ‚úÖ Collected {len(articles)} articles")

print(f"\nüéØ Total media examples: {len(media_examples)}")
```

---

## **Part 3: Business Privacy Policies (1,000 examples)**

```python
# collect_business_policies.py
"""
Collect public Vietnamese privacy policies
Practical, real-world examples
"""

VIETNAMESE_COMPANIES = [
    # E-commerce
    {'name': 'Shopee Vietnam', 'url': 'https://shopee.vn/legaldoc/privacy', 'sector': 'ecommerce'},
    {'name': 'Lazada Vietnam', 'url': 'https://www.lazada.vn/privacy-policy/', 'sector': 'ecommerce'},
    {'name': 'Tiki.vn', 'url': 'https://tiki.vn/dieu-khoan-su-dung', 'sector': 'ecommerce'},
    {'name': 'Sendo', 'url': 'https://www.sendo.vn/chinh-sach-bao-mat', 'sector': 'ecommerce'},
    
    # Technology
    {'name': 'VNG', 'url': 'https://www.vng.com.vn/privacy', 'sector': 'tech'},
    {'name': 'FPT', 'url': 'https://fpt.com.vn/chinh-sach-bao-mat', 'sector': 'tech'},
    {'name': 'Viettel', 'url': 'https://viettel.com.vn/privacy', 'sector': 'telecom'},
    
    # Banking
    {'name': 'VPBank', 'url': 'https://www.vpbank.com.vn/chinh-sach-bao-mat', 'sector': 'banking'},
    {'name': 'Techcombank', 'url': 'https://www.techcombank.com.vn/privacy', 'sector': 'banking'},
    {'name': 'VietinBank', 'url': 'https://www.vietinbank.vn/privacy-policy', 'sector': 'banking'},
]

def extract_privacy_policy_sections(company_info):
    """Extract Vietnamese privacy policy sections"""
    
    sections = []
    
    try:
        response = requests.get(company_info['url'])
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Find Vietnamese policy sections
        policy_sections = soup.find_all(['div', 'section'], class_=['policy', 'privacy'])
        
        for section in policy_sections:
            paragraphs = section.find_all('p')
            
            for para in paragraphs:
                text = para.get_text(strip=True)
                
                if len(text) >= 50:
                    sections.append({
                        'text': text,
                        'source': company_info['name'],
                        'sector': company_info['sector'],
                        'url': company_info['url'],
                        'type': 'privacy_policy',
                        'quality': 'medium-high'
                    })
    
    except Exception as e:
        print(f"Error extracting from {company_info['name']}: {e}")
    
    return sections

# Collect from all companies
business_examples = []

for company in VIETNAMESE_COMPANIES:
    print(f"üè¢ Collecting from: {company['name']}")
    sections = extract_privacy_policy_sections(company)
    business_examples.extend(sections)
    print(f"   ‚úÖ Collected {len(sections)} sections")

print(f"\nüéØ Total business examples: {len(business_examples)}")
```

---

## **Part 4: Synthetic Data Generation (1,000 examples)**

```python
# generate_synthetic_data.py
"""
Generate synthetic Vietnamese PDPL compliance data
Controlled quality, perfect regional balance
"""

import random

# 8 PDPL Categories (Vietnamese)
PDPL_CATEGORIES = {
    0: "T√≠nh h·ª£p ph√°p, c√¥ng b·∫±ng v√† minh b·∫°ch",
    1: "H·∫°n ch·∫ø m·ª•c ƒë√≠ch",
    2: "T·ªëi thi·ªÉu h√≥a d·ªØ li·ªáu",
    3: "T√≠nh ch√≠nh x√°c",
    4: "H·∫°n ch·∫ø l∆∞u tr·ªØ",
    5: "T√≠nh to√†n v·∫πn v√† b·∫£o m·∫≠t",
    6: "Tr√°ch nhi·ªám gi·∫£i tr√¨nh",
    7: "Quy·ªÅn c·ªßa ch·ªß th·ªÉ d·ªØ li·ªáu"
}

# Vietnamese Templates by Region
TEMPLATES = {
    0: {  # Lawfulness, fairness, transparency
        'bac': [
            "C√¥ng ty {company} c·∫ßn ph·∫£i thu th·∫≠p d·ªØ li·ªáu c√° nh√¢n m·ªôt c√°ch h·ª£p ph√°p, c√¥ng b·∫±ng v√† minh b·∫°ch theo quy ƒë·ªãnh c·ªßa PDPL 2025.",
            "C√°c t·ªï ch·ª©c c·∫ßn ph·∫£i ƒë·∫£m b·∫£o t√≠nh h·ª£p ph√°p khi thu th·∫≠p v√† x·ª≠ l√Ω d·ªØ li·ªáu c√° nh√¢n c·ªßa kh√°ch h√†ng.",
            "Doanh nghi·ªáp {company} c·∫ßn ph·∫£i th√¥ng b√°o r√µ r√†ng cho ch·ªß th·ªÉ d·ªØ li·ªáu v·ªÅ m·ª•c ƒë√≠ch thu th·∫≠p th√¥ng tin."
        ],
        'trung': [
            "C√¥ng ty {company} c·∫ßn thu th·∫≠p d·ªØ li·ªáu c√° nh√¢n h·ª£p ph√°p v√† c√¥ng khai theo lu·∫≠t PDPL.",
            "T·ªï ch·ª©c c·∫ßn b·∫£o ƒë·∫£m c√¥ng b·∫±ng trong vi·ªác x·ª≠ l√Ω th√¥ng tin kh√°ch h√†ng.",
            "Doanh nghi·ªáp {company} c·∫ßn cho bi·∫øt m·ª•c ƒë√≠ch thu th·∫≠p d·ªØ li·ªáu m·ªôt c√°ch minh b·∫°ch."
        ],
        'nam': [
            "C√¥ng ty {company} c·∫ßn thu th·∫≠p d·ªØ li·ªáu c·ªßa h·ªç m·ªôt c√°ch h·ª£p ph√°p v√† c√¥ng b·∫±ng.",
            "T·ªï ch·ª©c c·∫ßn ƒë·∫£m b·∫£o minh b·∫°ch khi x·ª≠ l√Ω th√¥ng tin c√° nh√¢n.",
            "Doanh nghi·ªáp {company} c·∫ßn cho kh√°ch h√†ng bi·∫øt t·∫°i sao h·ªç thu th·∫≠p d·ªØ li·ªáu."
        ]
    },
    1: {  # Purpose limitation
        'bac': [
            "D·ªØ li·ªáu c√° nh√¢n ch·ªâ ƒë∆∞·ª£c s·ª≠ d·ª•ng cho c√°c m·ª•c ƒë√≠ch ƒë√£ th√¥ng b√°o tr∆∞·ªõc cho ch·ªß th·ªÉ d·ªØ li·ªáu.",
            "C√¥ng ty {company} c·∫ßn ph·∫£i h·∫°n ch·∫ø vi·ªác s·ª≠ d·ª•ng d·ªØ li·ªáu theo ƒë√∫ng m·ª•c ƒë√≠ch ƒë√£ c√¥ng b·ªë.",
            "Kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng d·ªØ li·ªáu c√° nh√¢n cho c√°c m·ª•c ƒë√≠ch kh√°c ngo√†i nh·ªØng g√¨ ƒë√£ th√¥ng b√°o."
        ],
        'trung': [
            "D·ªØ li·ªáu ch·ªâ d√πng cho m·ª•c ƒë√≠ch ƒë√£ n√≥i v·ªõi ng∆∞·ªùi d√πng tr∆∞·ªõc ƒë√≥.",
            "C√¥ng ty {company} c·∫ßn gi·ªõi h·∫°n vi·ªác d√πng d·ªØ li·ªáu theo m·ª•c ƒë√≠ch ban ƒë·∫ßu.",
            "Kh√¥ng ƒë∆∞·ª£c d√πng th√¥ng tin c√° nh√¢n cho vi·ªác kh√°c."
        ],
        'nam': [
            "D·ªØ li·ªáu c·ªßa h·ªç ch·ªâ ƒë∆∞·ª£c d√πng cho m·ª•c ƒë√≠ch ƒë√£ n√≥i tr∆∞·ªõc.",
            "C√¥ng ty {company} c·∫ßn h·∫°n ch·∫ø d√πng d·ªØ li·ªáu ƒë√∫ng m·ª•c ƒë√≠ch.",
            "Kh√¥ng ƒë∆∞·ª£c d√πng th√¥ng tin c·ªßa h·ªç cho vi·ªác kh√°c."
        ]
    },
    # ... (continue for all 8 categories)
}

VIETNAMESE_COMPANIES = [
    'VNG', 'FPT', 'Viettel', 'Shopee', 'Lazada', 'Tiki',
    'VPBank', 'Techcombank', 'Grab', 'Gojek', 'MoMo', 'ZaloPay'
]

def generate_synthetic_dataset(num_samples=1000):
    """
    Generate synthetic Vietnamese PDPL dataset
    Balanced across categories and regions
    """
    
    dataset = []
    samples_per_category = num_samples // 8  # 125 per category
    samples_per_region = samples_per_category // 3  # ~42 per region
    
    for category in range(8):
        for region in ['bac', 'trung', 'nam']:
            templates = TEMPLATES.get(category, {}).get(region, [])
            
            if not templates:
                continue
            
            for _ in range(samples_per_region):
                template = random.choice(templates)
                company = random.choice(VIETNAMESE_COMPANIES)
                text = template.format(company=company)
                
                dataset.append({
                    'text': text,
                    'label': category,
                    'region': region,
                    'source': 'synthetic',
                    'category_name_vi': PDPL_CATEGORIES[category],
                    'quality': 'controlled'
                })
    
    return dataset

# Generate synthetic data
synthetic_data = generate_synthetic_dataset(1000)

# Save to JSONL
with open('vietnamese_pdpl_synthetic.jsonl', 'w', encoding='utf-8') as f:
    for item in synthetic_data:
        f.write(json.dumps(item, ensure_ascii=False) + '\n')

print(f"‚úÖ Generated {len(synthetic_data)} synthetic examples")
print(f"üìä Regional distribution:")
print(f"   B·∫Øc: {sum(1 for d in synthetic_data if d['region'] == 'bac')}")
print(f"   Trung: {sum(1 for d in synthetic_data if d['region'] == 'trung')}")
print(f"   Nam: {sum(1 for d in synthetic_data if d['region'] == 'nam')}")
```

---

## **Part 5: Crowdsourced Data (500 examples) - OPTIONAL** ‚≠ê

> **üí° Skip this for MVP**: Crowdsourcing is **optional** and costs $1,500. Start with 4,500 free examples (Parts 1-4), then add crowdsourcing later when funded.

### **Why Add Crowdsourcing? (Post-Funding)**

‚úÖ **Authentic regional variations** (real Vietnamese speakers)  
‚úÖ **+5-7% accuracy improvement** (90-93% ‚Üí 95-97%)  
‚úÖ **Natural language diversity** (not just formal legal text)  
‚úÖ **3x validation** (3 workers per example)  
‚úÖ **Production-ready quality** for enterprise customers

### **When to Add Crowdsourcing:**
- ‚úÖ After raising seed funding ($50K+)
- ‚úÖ Before production launch to enterprises
- ‚úÖ When accuracy matters for SLA commitments
- ‚ùå **Skip for MVP/demo** (use free data sources only)

---

```python
# crowdsource_vietnamese_data.py
"""
Crowdsource Vietnamese PDPL examples using MTurk
Diverse, authentic regional variations
‚ö†Ô∏è OPTIONAL: Only run this for production deployment
"""

import boto3

mturk = boto3.client('mturk', region_name='us-east-1')

VIETNAMESE_HIT_TEMPLATE = """
<HTMLQuestion xmlns="http://mechanicalturk.amazonaws.com/AWSMechanicalTurkDataSchemas/2011-11-11/HTMLQuestion.xsd">
    <HTMLContent><![CDATA[
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8"/>
        <title>Vi·∫øt c√¢u v·ªÅ PDPL 2025</title>
    </head>
    <body>
        <h2>Vi·∫øt c√¢u v·ªÅ tu√¢n th·ªß PDPL 2025 (Ti·∫øng Vi·ªát)</h2>
        
        <p><strong>Danh m·ª•c:</strong> {category_vi}</p>
        <p><strong>V√πng mi·ªÅn:</strong> {region}</p>
        
        <p>Vi·∫øt m·ªôt c√¢u ti·∫øng Vi·ªát v·ªÅ tu√¢n th·ªß b·∫£o v·ªá d·ªØ li·ªáu c√° nh√¢n:</p>
        
        <textarea name="pdpl_example" rows="4" cols="80" required placeholder="V√≠ d·ª•: C√¥ng ty c·∫ßn ph·∫£i b·∫£o v·ªá d·ªØ li·ªáu c√° nh√¢n m·ªôt c√°ch an to√†n..."></textarea>
        
        <p><em>L∆∞u √Ω:</em> C√¢u c·∫ßn r√µ r√†ng, ƒë√∫ng ng·ªØ ph√°p ti·∫øng Vi·ªát, v√† li√™n quan ƒë·∫øn danh m·ª•c tr√™n.</p>
    </body>
    </html>
    ]]></HTMLContent>
    <FrameHeight>500</FrameHeight>
</HTMLQuestion>
"""

def create_vietnamese_pdpl_hits(num_hits=500, reward=0.50):
    """
    Create HITs for Vietnamese PDPL data collection
    Target: Vietnamese workers only
    """
    
    hit_ids = []
    
    for i in range(num_hits):
        # Rotate through categories and regions
        category = i % 8
        region_idx = i % 3
        region = ['Mi·ªÅn B·∫Øc', 'Mi·ªÅn Trung', 'Mi·ªÅn Nam'][region_idx]
        category_vi = PDPL_CATEGORIES[category]
        
        hit_template = VIETNAMESE_HIT_TEMPLATE.format(
            category_vi=category_vi,
            region=region
        )
        
        response = mturk.create_hit(
            Title='Vi·∫øt c√¢u v·ªÅ tu√¢n th·ªß PDPL 2025 (Ti·∫øng Vi·ªát)',
            Description='Write a Vietnamese sentence about PDPL 2025 compliance',
            Keywords='vietnamese, legal, PDPL, data protection, compliance',
            Reward=str(reward),
            MaxAssignments=3,  # 3 Vietnamese workers per HIT
            LifetimeInSeconds=86400,  # 24 hours
            AssignmentDurationInSeconds=600,  # 10 minutes
            QualificationRequirements=[
                {
                    'QualificationTypeId': '00000000000000000071',  # Locale
                    'Comparator': 'EqualTo',
                    'LocaleValues': [{'Country': 'VN'}],  # Vietnam only
                    'ActionsGuarded': 'Accept'
                },
                {
                    'QualificationTypeId': '000000000000000000L0',  # Approval rate
                    'Comparator': 'GreaterThanOrEqualTo',
                    'IntegerValues': [95],  # 95%+ approval
                    'ActionsGuarded': 'Accept'
                }
            ],
            Question=hit_template
        )
        
        hit_ids.append(response['HIT']['HITId'])
        
        if (i + 1) % 50 == 0:
            print(f"‚úÖ Created {i + 1} HITs")
    
    print(f"\nüéØ Total HITs created: {len(hit_ids)}")
    print(f"üí∞ Total cost: ${len(hit_ids) * reward * 3:.2f} (including fees)")
    
    return hit_ids

# Create crowdsourcing tasks
hit_ids = create_vietnamese_pdpl_hits(num_hits=500, reward=0.50)
```

**Expected Cost**: 500 HITs √ó $0.50 √ó 3 workers = $750 + fees ‚âà **$1,500 total**

---

## **Data Quality Control**

```python
# quality_control.py
"""
Quality control for collected Vietnamese data
Ensures accuracy and regional balance
"""

def validate_vietnamese_text(text):
    """Validate Vietnamese text quality"""
    
    checks = {
        'min_length': len(text) >= 50,
        'has_vietnamese': any(char in text for char in '√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá'),
        'not_too_long': len(text) <= 500,
        'no_html': '<' not in text and '>' not in text
    }
    
    return all(checks.values()), checks

def balance_regional_distribution(dataset):
    """Ensure 33% B·∫Øc, 33% Trung, 34% Nam"""
    
    bac = [d for d in dataset if d.get('region') == 'bac']
    trung = [d for d in dataset if d.get('region') == 'trung']
    nam = [d for d in dataset if d.get('region') == 'nam']
    
    target = len(dataset) // 3
    
    balanced = []
    balanced.extend(random.sample(bac, min(target, len(bac))))
    balanced.extend(random.sample(trung, min(target, len(trung))))
    balanced.extend(random.sample(nam, min(target, len(nam))))
    
    return balanced

# Apply quality control
validated_dataset = []

for example in all_collected_data:
    is_valid, checks = validate_vietnamese_text(example['text'])
    
    if is_valid:
        validated_dataset.append(example)
    else:
        print(f"‚ùå Rejected: {checks}")

# Balance regions
final_dataset = balance_regional_distribution(validated_dataset)

print(f"\n‚úÖ Final dataset: {len(final_dataset)} examples")
```

---

## **Complete Collection Workflow**

### **üöÄ MVP Workflow (Recommended Start)**

```bash
# Week 1-2: Collect official documents
python collect_official_documents.py

# Week 2: Collect media articles
python collect_media_articles.py

# Week 3: Collect business policies
python collect_business_policies.py

# Week 3: Generate synthetic data (3 days)
python generate_synthetic_data.py

# Week 4: Quality control
python quality_control.py

# Week 4: Merge all sources (4,500 examples)
python merge_datasets.py --sources official,media,business,synthetic

# Week 4: Upload to S3
python upload_to_s3.py

# ‚úÖ MVP Dataset Ready: 4,500 examples, $0 cost, 90-93% accuracy
```

---

### **üéØ Production Workflow (Post-Funding)**

```bash
# Complete MVP workflow first (4,500 examples)
# Then add crowdsourcing:

# Week 5-6: Launch crowdsourcing (OPTIONAL)
python crowdsource_vietnamese_data.py

# Week 6: Wait for HITs completion
python check_mturk_results.py

# Week 6: Quality control on crowdsourced data
python quality_control.py --source crowdsourced

# Week 6: Merge all sources (5,000 examples)
python merge_datasets.py --sources official,media,business,synthetic,crowdsourced

# Week 6: Re-upload to S3
python upload_to_s3.py

# ‚úÖ Production Dataset Ready: 5,000 examples, $1,500 cost, 95-97% accuracy
```

---

### **üí° Alternative: University Partnership (Free Crowdsourcing)**

```bash
# Instead of MTurk, partner with Vietnamese universities
# Cost: $0, Timeline: 6-8 weeks

# Contact universities:
python contact_universities.py --universities hanoi_law,hcmc_law,ftu

# Coordinate with professors:
python setup_student_tasks.py --target_examples 500

# Review student submissions:
python review_submissions.py --min_quality 0.8

# ‚úÖ Free alternative to MTurk (slower but $0 cost)
```

---

## **Expected Results**

### **üöÄ MVP Results (No Crowdsourcing)**

**Dataset Composition:**
- **Total Examples**: 4,500
- **Format**: JSONL (UTF-8 encoded)
- **Regional Balance**: 1,485 B·∫Øc, 1,485 Trung, 1,530 Nam
- **Category Balance**: ~563 per PDPL category
- **Quality Mix**: 33% High, 45% Medium-High, 22% Controlled

**Timeline & Cost:**
- **Total Time**: 3-4 weeks
- **Total Cost**: **$0** ‚úÖ
- **Expected Accuracy**: **90-93%**
- **Good for**: Investor demo, MVP, proof of concept

---

### **üéØ Production Results (With Crowdsourcing)**

**Dataset Composition:**
- **Total Examples**: 5,000
- **Format**: JSONL (UTF-8 encoded)
- **Regional Balance**: 1,650 B·∫Øc, 1,650 Trung, 1,700 Nam
- **Category Balance**: ~625 per PDPL category
- **Quality Mix**: 30% High, 40% Medium-High, 20% Controlled, 10% Authentic

**Timeline & Cost:**
- **Total Time**: 4-6 weeks
- **Total Cost**: **$1,500**
- **Expected Accuracy**: **95-97%**
- **Good for**: Enterprise customers, production deployment, SLA commitments

---

### **üìä Comparison: MVP vs. Production**

| Metric | MVP (Free) | Production (+$1,500) | Difference |
|--------|-----------|---------------------|------------|
| **Examples** | 4,500 | 5,000 | +500 (+11%) |
| **Accuracy** | 90-93% | 95-97% | +5-7% |
| **Cost** | $0 | $1,500 | +$1,500 |
| **Timeline** | 3-4 weeks | 4-6 weeks | +1-2 weeks |
| **Authenticity** | Medium | High | Better |
| **Regional Diversity** | Good | Excellent | Better |
| **Use Case** | Demo, MVP | Production | - |

**üí° Recommendation**: Start with MVP (free), validate with investors/users, then upgrade to Production when funded.

---

## üè¢ Company Lists: Web Scraping vs. Dataset Generation

### **Important Distinction**

This guide contains a **hardcoded company list** for **one-time web scraping** (lines 239-249). This is **different** from the **Dynamic Company Registry** used for ongoing dataset generation.

### **Two Different Purposes:**

#### **1. Web Scraping List (This Guide) - ONE-TIME USE**

```python
VIETNAMESE_COMPANIES = [
    {'name': 'Shopee Vietnam', 'url': 'https://shopee.vn/legaldoc/privacy'},
    {'name': 'Lazada Vietnam', 'url': 'https://www.lazada.vn/privacy-policy/'},
    # ... 10-20 companies
]
```

**Purpose**: Collect initial training data from public privacy policies  
**Frequency**: Once (during data collection phase)  
**Scope**: Limited to companies with accessible websites  
**Output**: Real-world Vietnamese compliance text  
**Maintenance**: Static list (no updates needed after collection)  

**Why Hardcoded is OK Here**:
- ‚úÖ One-time data collection activity
- ‚úÖ Small list (10-20 companies)
- ‚úÖ URL-specific (not reusable)
- ‚úÖ Quality filter (only companies with good policies)

---

#### **2. Dynamic Company Registry - ONGOING GENERATION**

```python
# From config/company_registry.json (150+ companies)
COMPANIES_ALL = load_from_registry()  # 150+ Vietnamese companies
company = select_company_for_template(template)
text = template.format(company=company)
```

**Purpose**: Generate synthetic training data at scale  
**Frequency**: Continuous (every training run)  
**Scope**: All Vietnamese companies (150+ and growing)  
**Output**: Company-normalized training samples  
**Maintenance**: Hot-reload updates (zero downtime)  

**Why Dynamic Registry Here**:
- ‚úÖ Ongoing dataset generation
- ‚úÖ Large list (150+ companies, growing)
- ‚úÖ Template-based (reusable)
- ‚úÖ Zero retraining for new companies

---

### **How They Work Together:**

```
STEP 1: DATA COLLECTION (This Guide)
‚îú‚îÄ Scrape 10-20 company websites (hardcoded list)
‚îú‚îÄ Extract 1,000 real Vietnamese privacy policy examples
‚îú‚îÄ Feed into company_registry.json (enrich registry)
‚îî‚îÄ One-time activity (MVP phase)

STEP 2: DATASET GENERATION (Dynamic Registry)
‚îú‚îÄ Load 150+ companies from registry
‚îú‚îÄ Generate 150,300 synthetic samples
‚îú‚îÄ Normalize to [COMPANY] token
‚îú‚îÄ Train company-agnostic models
‚îî‚îÄ Ongoing activity (production phase)

STEP 3: ADD NEW COMPANIES (Dynamic Registry Only)
‚îú‚îÄ Update company_registry.json (5 minutes)
‚îú‚îÄ No web scraping needed
‚îú‚îÄ No model retraining needed
‚îî‚îÄ Future-proof scalability
```

### **When to Update Each List:**

| Scenario | Web Scraping List | Dynamic Registry |
|----------|------------------|------------------|
| **Initial MVP** | ‚úÖ Update once | ‚úÖ Create initial |
| **Add 1 company** | ‚ùå No update | ‚úÖ Update JSON |
| **Add 10 companies** | ‚ùå No update | ‚úÖ Update JSON |
| **Scrape new policies** | ‚úÖ Update if needed | ‚ùå No update |
| **Generate new dataset** | ‚ùå No update | ‚úÖ Use registry |

### **Best Practice Workflow:**

```bash
# Phase 1: Initial Data Collection (Once)
python collect_business_policies.py  # Uses hardcoded VIETNAMESE_COMPANIES

# Phase 2: Enrich Company Registry
python enrich_registry_from_scraped_data.py  # Add scraped companies to registry

# Phase 3: Generate Datasets (Ongoing)
python generate_hard_dataset.py --use-company-registry  # Uses Dynamic Registry

# Phase 4: Add New Companies (Anytime)
# Edit config/company_registry.json manually
# OR use Admin API
curl -X POST /api/v1/admin/companies/add -d '{...}'
```

### **Implementation Reference**

For complete Dynamic Company Registry architecture:
- **`VeriAIDPO_Dynamic_Company_Registry_Implementation.md`** - Full implementation plan
- **`VeriAIDPO_Hard_Dataset_Generation_Guide.md`** - Dataset generation with registry
- **`config/company_registry.json`** - 150+ Vietnamese companies database

### **Key Takeaway**

‚úÖ **Web Scraping List**: Static, one-time, small (10-20 companies)  
‚úÖ **Dynamic Registry**: Scalable, ongoing, large (150+ companies)  
‚úÖ **Both Serve Different Purposes**: Don't confuse them!  
‚úÖ **Scraped Data Enriches Registry**: They work together sequentially  

---

*Document Version: 2.0 (Company List Clarification)*  
*Last Updated: October 14, 2025*  
*Focus: Hybrid Data Collection Strategy*  
*Target: Production-Quality Vietnamese PDPL Dataset*  
*Changes: Added clarification distinguishing web scraping list from Dynamic Company Registry*

