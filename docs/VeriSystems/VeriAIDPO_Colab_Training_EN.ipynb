{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1e2796c",
   "metadata": {},
   "source": [
    "# VeriAIDPO - English Model Training Pipeline\n",
    "## BERT-base-uncased for English PDPL 2025 Compliance\n",
    "\n",
    "**Enterprise-Ready AI Training for English Data Protection Requests**\n",
    "\n",
    "---\n",
    "\n",
    "### Training Configuration:\n",
    "- **Model**: BERT-base-uncased (110M parameters)\n",
    "- **Dataset**: 5,000 English PDPL templates (625 per category)\n",
    "- **Architecture**: Separate model (Option A - Bilingual Strategy)\n",
    "- **Target Accuracy**: 88-92% (production-grade)\n",
    "- **Model Size**: ~440MB\n",
    "- **Categories**: 8 PDPL 2025 compliance categories\n",
    "\n",
    "### Expected Performance:\n",
    "- **Training Time**: 2-3 hours on T4 GPU\n",
    "- **Inference Speed**: 40-80ms per request\n",
    "- **Target Accuracy**: 88-92% on test set\n",
    "- **Confidence**: 88-95% average\n",
    "\n",
    "### Quality Assurance:\n",
    "- Zero data leakage detection\n",
    "- Template diversity analysis (100+ unique structures)\n",
    "- Overfitting prevention (>=95% accuracy early stop)\n",
    "- Underfitting detection (<=40% by epoch 2)\n",
    "- Reserved company sets for train/val/test isolation\n",
    "\n",
    "### Bilingual Integration:\n",
    "- Companion to Vietnamese PhoBERT model\n",
    "- Completely independent dataset (no overlap)\n",
    "- Same 8 PDPL categories (English translations)\n",
    "- Exports to VeriSyntra backend (veriaidpo_en)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1be7d4",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup & GPU Validation\n",
    "\n",
    "**Enterprise-grade environment setup with comprehensive validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a84168",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70, flush=True)\n",
    "print(\"STEP 1: ENVIRONMENT SETUP & GPU VALIDATION\", flush=True)\n",
    "print(\"=\"*70 + \"\\n\", flush=True)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Check if running on Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(f\"Environment: {'Google Colab' if IN_COLAB else 'Local/Other'}\", flush=True)\n",
    "print(f\"Python Version: {sys.version}\", flush=True)\n",
    "print(f\"Execution Start: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "# Install required packages\n",
    "print(\"Installing required packages...\", flush=True)\n",
    "!pip install -q transformers datasets torch scikit-learn matplotlib seaborn\n",
    "print(\"Package installation complete!\\n\", flush=True)\n",
    "\n",
    "# Import libraries\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TrainerCallback\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import json\n",
    "import random\n",
    "from typing import List, Dict, Tuple, Set\n",
    "from collections import defaultdict, Counter\n",
    "import hashlib\n",
    "import re\n",
    "\n",
    "print(\"Library Import Status:\", flush=True)\n",
    "print(f\"  - PyTorch: {torch.__version__}\", flush=True)\n",
    "print(f\"  - Transformers: {transformers.__version__}\", flush=True)\n",
    "print(f\"  - NumPy: {np.__version__}\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "# GPU Validation\n",
    "print(\"GPU Validation:\", flush=True)\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"  GPU Available: {gpu_name}\", flush=True)\n",
    "    print(f\"  GPU Memory: {gpu_memory:.2f} GB\", flush=True)\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"  Status: READY FOR TRAINING\", flush=True)\n",
    "else:\n",
    "    print(\"  WARNING: No GPU detected. Training will be slow on CPU.\", flush=True)\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"  Status: CPU MODE (Not recommended for production)\", flush=True)\n",
    "\n",
    "print(flush=True)\n",
    "print(\"Environment setup complete!\\n\", flush=True)\n",
    "print(\"=\"*70, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4ad1a2",
   "metadata": {},
   "source": [
    "## Step 2: Language Configuration & Hyperparameters\n",
    "\n",
    "**Dynamic configuration for English BERT model training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd144580",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70, flush=True)\n",
    "print(\"STEP 2: LANGUAGE CONFIGURATION & HYPERPARAMETERS\", flush=True)\n",
    "print(\"=\"*70 + \"\\n\", flush=True)\n",
    "\n",
    "# === LANGUAGE CONFIGURATION ===\n",
    "LANGUAGE = \"en\"  # English training\n",
    "MODEL_NAME = \"bert-base-uncased\"  # BERT for English\n",
    "MODEL_DISPLAY_NAME = \"BERT-base-uncased\"\n",
    "\n",
    "# === DATASET CONFIGURATION ===\n",
    "DATASET_TARGET = 5000  # Total samples\n",
    "NUM_CATEGORIES = 8  # PDPL categories\n",
    "SAMPLES_PER_CATEGORY = DATASET_TARGET // NUM_CATEGORIES  # 625 per category\n",
    "\n",
    "# === FILE PATHS ===\n",
    "OUTPUT_DATASET_FILE = \"english_pdpl_complete.jsonl\"\n",
    "TRAIN_FILE = \"english_pdpl_train.jsonl\"\n",
    "VAL_FILE = \"english_pdpl_val.jsonl\"\n",
    "TEST_FILE = \"english_pdpl_test.jsonl\"\n",
    "MODEL_SAVE_DIR = \"./veriaidpo_en_model\"\n",
    "OUTPUT_DIR = \"./\"  # Current directory\n",
    "MAX_LENGTH = 256  # Token sequence length\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# === HYPERPARAMETERS (English-optimized) ===\n",
    "TRAINING_CONFIG = {\n",
    "    # Model configuration\n",
    "    'model_name': MODEL_NAME,\n",
    "    'hidden_dropout_prob': 0.20,  # Lower than Vietnamese (English easier)\n",
    "    'attention_probs_dropout_prob': 0.20,\n",
    "    'classifier_dropout': 0.20,\n",
    "    \n",
    "    # Training arguments\n",
    "    'num_train_epochs': 8,  # Less than Vietnamese (faster convergence)\n",
    "    'learning_rate': 3e-5,  # Standard BERT learning rate\n",
    "    'per_device_train_batch_size': 8,\n",
    "    'per_device_eval_batch_size': 16,\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'label_smoothing_factor': 0.10,  # Moderate smoothing\n",
    "    'lr_scheduler_type': 'cosine',\n",
    "    'save_strategy': 'epoch',\n",
    "    'evaluation_strategy': 'epoch',\n",
    "    'load_best_model_at_end': True,\n",
    "    'metric_for_best_model': 'eval_accuracy',\n",
    "    \n",
    "    # Early stopping thresholds (English)\n",
    "    'early_high_accuracy_threshold': 0.90,  # 90% (vs 92% for Vietnamese)\n",
    "    'extreme_overfitting_threshold': 0.95,\n",
    "    'underfitting_threshold': 0.40,\n",
    "    'patience': 3,\n",
    "    \n",
    "    # File paths and directories\n",
    "    'output_dir': OUTPUT_DIR,\n",
    "    'model_save_dir': MODEL_SAVE_DIR,\n",
    "    'train_file': TRAIN_FILE,\n",
    "    'val_file': VAL_FILE,\n",
    "    'test_file': TEST_FILE,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'seed': RANDOM_SEED,\n",
    "    \n",
    "    # Nested hyperparameters dictionary for SmartTrainingCallback\n",
    "    'hyperparameters': {\n",
    "        'early_high_accuracy_threshold': 0.90,\n",
    "        'extreme_overfitting_threshold': 0.95,\n",
    "    }\n",
    "}\n",
    "\n",
    "# === DATA SPLIT CONFIGURATION ===\n",
    "DATA_SPLIT = {\n",
    "    'train': 0.70,  # 3,500 samples\n",
    "    'val': 0.15,    # 750 samples\n",
    "    'test': 0.15    # 750 samples\n",
    "}\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# Display configuration\n",
    "print(\"Configuration Summary:\", flush=True)\n",
    "print(f\"  Language: {LANGUAGE.upper()}\", flush=True)\n",
    "print(f\"  Model: {MODEL_DISPLAY_NAME}\", flush=True)\n",
    "print(f\"  Target Dataset Size: {DATASET_TARGET:,} samples\", flush=True)\n",
    "print(f\"  Samples per Category: {SAMPLES_PER_CATEGORY} samples\", flush=True)\n",
    "print(f\"  Number of Categories: {NUM_CATEGORIES}\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "print(\"Hyperparameters:\", flush=True)\n",
    "print(f\"  Dropout: {TRAINING_CONFIG['hidden_dropout_prob']}\", flush=True)\n",
    "print(f\"  Learning Rate: {TRAINING_CONFIG['learning_rate']}\", flush=True)\n",
    "print(f\"  Label Smoothing: {TRAINING_CONFIG['label_smoothing_factor']}\", flush=True)\n",
    "print(f\"  Epochs: {TRAINING_CONFIG['num_train_epochs']}\", flush=True)\n",
    "print(f\"  Batch Size: {TRAINING_CONFIG['per_device_train_batch_size']}\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "print(\"Data Split:\", flush=True)\n",
    "print(f\"  Train: {DATA_SPLIT['train']*100:.0f}% ({int(DATASET_TARGET * DATA_SPLIT['train']):,} samples)\", flush=True)\n",
    "print(f\"  Validation: {DATA_SPLIT['val']*100:.0f}% ({int(DATASET_TARGET * DATA_SPLIT['val']):,} samples)\", flush=True)\n",
    "print(f\"  Test: {DATA_SPLIT['test']*100:.0f}% ({int(DATASET_TARGET * DATA_SPLIT['test']):,} samples)\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "print(\"Configuration complete!\\n\", flush=True)\n",
    "print(\"=\"*70, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422b3989",
   "metadata": {},
   "source": [
    "## Step 3: PDPL Categories Definition\n",
    "\n",
    "**8 PDPL 2025 compliance categories (bilingual - same as Vietnamese model)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b23823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70, flush=True)\n",
    "print(\"STEP 3: PDPL CATEGORIES DEFINITION\", flush=True)\n",
    "print(\"=\"*70 + \"\\n\", flush=True)\n",
    "\n",
    "# Enhanced PDPL 2025 Categories (Bilingual)\n",
    "# Same categories as Vietnamese model for consistency\n",
    "PDPL_CATEGORIES = {\n",
    "    0: {\n",
    "        \"vi\": \"Tinh hop phap, cong bang va minh bach\",\n",
    "        \"en\": \"Lawfulness, fairness and transparency\"\n",
    "    },\n",
    "    1: {\n",
    "        \"vi\": \"Han che muc dich\",\n",
    "        \"en\": \"Purpose limitation\"\n",
    "    },\n",
    "    2: {\n",
    "        \"vi\": \"Toi thieu hoa du lieu\",\n",
    "        \"en\": \"Data minimisation\"\n",
    "    },\n",
    "    3: {\n",
    "        \"vi\": \"Tinh chinh xac\",\n",
    "        \"en\": \"Accuracy\"\n",
    "    },\n",
    "    4: {\n",
    "        \"vi\": \"Han che luu tru\",\n",
    "        \"en\": \"Storage limitation\"\n",
    "    },\n",
    "    5: {\n",
    "        \"vi\": \"Tinh toan ven va bao mat\",\n",
    "        \"en\": \"Integrity and confidentiality\"\n",
    "    },\n",
    "    6: {\n",
    "        \"vi\": \"Trach nhiem giai trinh\",\n",
    "        \"en\": \"Accountability\"\n",
    "    },\n",
    "    7: {\n",
    "        \"vi\": \"Quyen cua chu the du lieu\",\n",
    "        \"en\": \"Data subject rights\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display categories\n",
    "print(\"PDPL 2025 Categories (English):\", flush=True)\n",
    "print(flush=True)\n",
    "for cat_id, names in PDPL_CATEGORIES.items():\n",
    "    print(f\"  Category {cat_id}: {names['en']}\", flush=True)\n",
    "    print(f\"    Vietnamese: {names['vi']}\", flush=True)\n",
    "    print(flush=True)\n",
    "\n",
    "print(f\"Total Categories: {len(PDPL_CATEGORIES)}\", flush=True)\n",
    "print(f\"Target per Category: {SAMPLES_PER_CATEGORY} samples\\n\", flush=True)\n",
    "print(\"=\"*70, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e1b5fb",
   "metadata": {},
   "source": [
    "## Step 4: English Template Generator\n",
    "\n",
    "**Generate diverse English PDPL templates with zero Vietnamese overlap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f45c54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70, flush=True)\n",
    "print(\"STEP 4: ENGLISH TEMPLATE GENERATOR\", flush=True)\n",
    "print(\"=\"*70 + \"\\n\", flush=True)\n",
    "\n",
    "# English companies (same names as Vietnamese but English contexts)\n",
    "ENGLISH_COMPANIES = {\n",
    "    'north': [\n",
    "        'VNG', 'FPT', 'VNPT', 'Viettel', 'Vingroup', 'VietinBank', \n",
    "        'Agribank', 'BIDV', 'MB Bank', 'ACB', 'VPBank', 'TPBank'\n",
    "    ],\n",
    "    'central': [\n",
    "        'Vinamilk', 'Hoa Phat', 'Petrolimex', 'PVN', 'EVN', \n",
    "        'Vinatex', 'DHG Pharma', 'Hau Giang Pharma'\n",
    "    ],\n",
    "    'south': [\n",
    "        'Shopee VN', 'Lazada VN', 'Tiki', 'Grab VN', 'MoMo', 'ZaloPay', \n",
    "        'Techcombank', 'VCB', 'Sacombank', 'HDBank'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# English business contexts\n",
    "BUSINESS_CONTEXTS_EN = {\n",
    "    'banking': [\n",
    "        'account', 'transaction', 'credit card', 'loan', 'deposit', \n",
    "        'transfer', 'investment', 'insurance', 'mortgage', 'credit'\n",
    "    ],\n",
    "    'ecommerce': [\n",
    "        'order', 'payment', 'delivery', 'product', 'promotion', \n",
    "        'review', 'cart', 'voucher', 'refund', 'return'\n",
    "    ],\n",
    "    'healthcare': [\n",
    "        'medical record', 'consultation', 'prescription', 'insurance', \n",
    "        'test', 'diagnosis', 'treatment', 'surgery', 'follow-up', 'vaccine'\n",
    "    ],\n",
    "    'education': [\n",
    "        'student', 'grade', 'tuition', 'certificate', 'course', \n",
    "        'degree', 'exam', 'scholarship', 'enrollment', 'schedule'\n",
    "    ],\n",
    "    'technology': [\n",
    "        'application', 'account', 'data', 'security', 'service', \n",
    "        'software', 'login', 'password', 'API', 'cloud'\n",
    "    ],\n",
    "    'insurance': [\n",
    "        'policy', 'benefit', 'claim', 'premium', 'contract', \n",
    "        'claim request', 'risk assessment', 'reinsurance'\n",
    "    ],\n",
    "    'telecommunications': [\n",
    "        'call', 'message', 'data', 'roaming', 'charge', \n",
    "        'subscription', 'network', 'phone number', 'internet'\n",
    "    ],\n",
    "    'logistics': [\n",
    "        'shipping', 'delivery', 'warehouse', 'tracking', 'fee', \n",
    "        'packaging', 'export', 'import', 'logistics'\n",
    "    ]\n",
    "}\n",
    "\n",
    "class EnglishTemplateGenerator:\n",
    "    \"\"\"Generate diverse English PDPL templates with zero overlap\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.companies = ENGLISH_COMPANIES\n",
    "        self.contexts = BUSINESS_CONTEXTS_EN\n",
    "        self.generated_templates = set()\n",
    "        self.template_hashes = set()\n",
    "        \n",
    "        # Sentence structure types (English grammar)\n",
    "        self.structure_types = ['simple', 'compound', 'complex']\n",
    "        \n",
    "        # Formality levels\n",
    "        self.formality_levels = ['formal', 'business', 'standard']\n",
    "        \n",
    "        # Modal verbs for variation\n",
    "        self.modals = ['must', 'shall', 'should', 'will', 'can', 'may']\n",
    "        \n",
    "        # Verb variations\n",
    "        self.verbs = {\n",
    "            'process': ['process', 'handle', 'manage', 'deal with'],\n",
    "            'collect': ['collect', 'gather', 'obtain', 'acquire'],\n",
    "            'store': ['store', 'retain', 'keep', 'maintain'],\n",
    "            'delete': ['delete', 'remove', 'erase', 'eliminate'],\n",
    "            'protect': ['protect', 'safeguard', 'secure', 'shield'],\n",
    "            'ensure': ['ensure', 'guarantee', 'maintain', 'establish']\n",
    "        }\n",
    "    \n",
    "    def get_category_templates(self, category_id: int) -> Dict[str, List[str]]:\n",
    "        \"\"\"Get template patterns for each PDPL category\"\"\"\n",
    "        \n",
    "        templates = {\n",
    "            0: {  # Lawfulness, fairness and transparency\n",
    "                'simple': [\n",
    "                    \"{company} {modal} process {context} data lawfully and fairly.\",\n",
    "                    \"The company {company} ensures transparency in {context} processing.\",\n",
    "                    \"{company} provides clear information about {context} data usage.\",\n",
    "                    \"Lawful processing of {context} is required by {company}.\",\n",
    "                    \"{company} commits to fair data practices for {context}.\",\n",
    "                    \"{company} {modal} handle {context} data with transparency.\",\n",
    "                    \"Fair and lawful {context} processing is ensured by {company}.\"\n",
    "                ],\n",
    "                'compound': [\n",
    "                    \"{company} processes {context} data lawfully and provides transparent information.\",\n",
    "                    \"The company {company} ensures fairness but also complies with legal requirements for {context}.\",\n",
    "                    \"{company} maintains transparency and lawful processing of {context} data.\",\n",
    "                    \"{company} {modal} process {context} fairly and ensure legal compliance.\",\n",
    "                    \"Legal compliance is maintained and {context} data is processed transparently by {company}.\"\n",
    "                ],\n",
    "                'complex': [\n",
    "                    \"To ensure lawfulness, {company} establishes clear legal basis for {context} processing.\",\n",
    "                    \"When processing {context} data, {company} {modal} demonstrate compliance with legal requirements.\",\n",
    "                    \"Although complex, {company} commits to maintaining transparency in {context} processing.\",\n",
    "                    \"Before processing {context}, {company} verifies legal grounds and ensures fairness.\",\n",
    "                    \"If {context} data is processed, {company} guarantees lawful and transparent handling.\"\n",
    "                ]\n",
    "            },\n",
    "            1: {  # Purpose limitation\n",
    "                'simple': [\n",
    "                    \"{company} limits {context} data to specified purposes.\",\n",
    "                    \"The company {company} restricts {context} use to stated goals.\",\n",
    "                    \"{company} {modal} collect {context} for defined purposes only.\",\n",
    "                    \"Purpose limitation applies to all {context} data at {company}.\",\n",
    "                    \"{company} ensures {context} is used for specified purposes.\",\n",
    "                    \"Data collection for {context} is purpose-limited by {company}.\"\n",
    "                ],\n",
    "                'compound': [\n",
    "                    \"{company} collects {context} data and restricts use to stated purposes.\",\n",
    "                    \"The company {company} defines purposes and limits {context} data usage accordingly.\",\n",
    "                    \"{company} {modal} specify purposes but ensure {context} is not used beyond them.\",\n",
    "                    \"Purpose specification is done and {context} usage is limited by {company}.\"\n",
    "                ],\n",
    "                'complex': [\n",
    "                    \"When collecting {context} data, {company} ensures purpose limitation is enforced.\",\n",
    "                    \"To prevent misuse, {company} restricts {context} data to originally stated purposes.\",\n",
    "                    \"Although {context} has multiple uses, {company} limits processing to defined goals.\",\n",
    "                    \"Before using {context}, {company} verifies alignment with specified purposes.\"\n",
    "                ]\n",
    "            },\n",
    "            2: {  # Data minimisation\n",
    "                'simple': [\n",
    "                    \"{company} collects only necessary {context} data.\",\n",
    "                    \"The company {company} minimizes {context} data collection.\",\n",
    "                    \"{company} {modal} limit {context} to essential information only.\",\n",
    "                    \"Data minimisation principles apply to {context} at {company}.\",\n",
    "                    \"{company} ensures minimal {context} data is collected.\",\n",
    "                    \"Only required {context} information is gathered by {company}.\"\n",
    "                ],\n",
    "                'compound': [\n",
    "                    \"{company} evaluates necessity and collects minimal {context} data.\",\n",
    "                    \"The company {company} reviews requirements and minimizes {context} collection.\",\n",
    "                    \"{company} {modal} assess needs but collect only essential {context} data.\",\n",
    "                    \"Necessity is evaluated and {context} minimisation is applied by {company}.\"\n",
    "                ],\n",
    "                'complex': [\n",
    "                    \"To reduce data volume, {company} collects only necessary {context} information.\",\n",
    "                    \"When gathering {context}, {company} ensures data minimisation is implemented.\",\n",
    "                    \"Although more {context} could be collected, {company} limits to essentials.\",\n",
    "                    \"Before collecting {context}, {company} verifies necessity and minimizes data.\"\n",
    "                ]\n",
    "            },\n",
    "            3: {  # Accuracy\n",
    "                'simple': [\n",
    "                    \"{company} ensures {context} data is accurate and up-to-date.\",\n",
    "                    \"The company {company} maintains accurate {context} records.\",\n",
    "                    \"{company} {modal} verify {context} data accuracy regularly.\",\n",
    "                    \"Data accuracy is maintained for {context} by {company}.\",\n",
    "                    \"{company} updates {context} information to ensure accuracy.\",\n",
    "                    \"Accurate {context} data is guaranteed by {company}.\"\n",
    "                ],\n",
    "                'compound': [\n",
    "                    \"{company} verifies {context} data and maintains accuracy standards.\",\n",
    "                    \"The company {company} reviews records and ensures {context} is accurate.\",\n",
    "                    \"{company} {modal} check {context} but also update when needed.\",\n",
    "                    \"Verification is performed and {context} accuracy is ensured by {company}.\"\n",
    "                ],\n",
    "                'complex': [\n",
    "                    \"To ensure reliability, {company} verifies {context} data accuracy regularly.\",\n",
    "                    \"When managing {context}, {company} implements accuracy verification processes.\",\n",
    "                    \"Although {context} changes over time, {company} maintains data accuracy.\",\n",
    "                    \"Before using {context}, {company} confirms data is accurate and current.\"\n",
    "                ]\n",
    "            },\n",
    "            4: {  # Storage limitation\n",
    "                'simple': [\n",
    "                    \"{company} limits {context} data storage duration.\",\n",
    "                    \"The company {company} retains {context} for limited periods.\",\n",
    "                    \"{company} {modal} delete {context} data after specified time.\",\n",
    "                    \"Storage limitation applies to {context} at {company}.\",\n",
    "                    \"{company} ensures {context} is not kept longer than necessary.\",\n",
    "                    \"Retention periods for {context} are enforced by {company}.\"\n",
    "                ],\n",
    "                'compound': [\n",
    "                    \"{company} sets retention periods and deletes {context} accordingly.\",\n",
    "                    \"The company {company} defines storage limits and removes old {context} data.\",\n",
    "                    \"{company} {modal} retain {context} temporarily but delete after use.\",\n",
    "                    \"Time limits are set and {context} deletion is enforced by {company}.\"\n",
    "                ],\n",
    "                'complex': [\n",
    "                    \"To prevent excessive storage, {company} deletes {context} after retention period.\",\n",
    "                    \"When storing {context}, {company} ensures compliance with time limitations.\",\n",
    "                    \"Although {context} may be needed later, {company} limits storage duration.\",\n",
    "                    \"After retention period expires, {company} securely deletes {context} data.\"\n",
    "                ]\n",
    "            },\n",
    "            5: {  # Integrity and confidentiality\n",
    "                'simple': [\n",
    "                    \"{company} protects {context} data integrity and confidentiality.\",\n",
    "                    \"The company {company} secures {context} against unauthorized access.\",\n",
    "                    \"{company} {modal} implement security measures for {context}.\",\n",
    "                    \"Data security is maintained for {context} by {company}.\",\n",
    "                    \"{company} ensures {context} confidentiality through encryption.\",\n",
    "                    \"Integrity of {context} data is protected by {company}.\"\n",
    "                ],\n",
    "                'compound': [\n",
    "                    \"{company} encrypts {context} data and prevents unauthorized access.\",\n",
    "                    \"The company {company} implements controls and protects {context} integrity.\",\n",
    "                    \"{company} {modal} secure {context} but also monitor for breaches.\",\n",
    "                    \"Security measures are applied and {context} confidentiality is ensured by {company}.\"\n",
    "                ],\n",
    "                'complex': [\n",
    "                    \"To prevent data breaches, {company} implements security controls for {context}.\",\n",
    "                    \"When handling {context}, {company} ensures integrity and confidentiality.\",\n",
    "                    \"Although threats exist, {company} protects {context} through robust security.\",\n",
    "                    \"Before processing {context}, {company} verifies security measures are active.\"\n",
    "                ]\n",
    "            },\n",
    "            6: {  # Accountability\n",
    "                'simple': [\n",
    "                    \"{company} demonstrates accountability for {context} data processing.\",\n",
    "                    \"The company {company} maintains records of {context} activities.\",\n",
    "                    \"{company} {modal} document {context} processing decisions.\",\n",
    "                    \"Accountability measures are implemented for {context} by {company}.\",\n",
    "                    \"{company} takes responsibility for {context} data handling.\",\n",
    "                    \"Documentation of {context} processing is maintained by {company}.\"\n",
    "                ],\n",
    "                'compound': [\n",
    "                    \"{company} documents decisions and demonstrates accountability for {context}.\",\n",
    "                    \"The company {company} maintains logs and shows responsibility for {context}.\",\n",
    "                    \"{company} {modal} record activities but also ensure accountability.\",\n",
    "                    \"Records are kept and {context} accountability is demonstrated by {company}.\"\n",
    "                ],\n",
    "                'complex': [\n",
    "                    \"To demonstrate compliance, {company} maintains detailed {context} processing records.\",\n",
    "                    \"When processing {context}, {company} ensures accountability through documentation.\",\n",
    "                    \"Although complex, {company} takes full responsibility for {context} handling.\",\n",
    "                    \"Before making decisions about {context}, {company} documents the rationale.\"\n",
    "                ]\n",
    "            },\n",
    "            7: {  # Data subject rights\n",
    "                'simple': [\n",
    "                    \"{company} respects data subject rights for {context}.\",\n",
    "                    \"The company {company} enables {context} data access requests.\",\n",
    "                    \"{company} {modal} provide {context} data upon user request.\",\n",
    "                    \"User rights are honored for {context} by {company}.\",\n",
    "                    \"{company} facilitates {context} data deletion requests.\",\n",
    "                    \"Data subject rights regarding {context} are protected by {company}.\"\n",
    "                ],\n",
    "                'compound': [\n",
    "                    \"{company} receives requests and provides {context} data access.\",\n",
    "                    \"The company {company} honors rights and enables {context} correction.\",\n",
    "                    \"{company} {modal} process requests but also verify user identity.\",\n",
    "                    \"Requests are handled and {context} rights are respected by {company}.\"\n",
    "                ],\n",
    "                'complex': [\n",
    "                    \"To respect user rights, {company} enables {context} data access and deletion.\",\n",
    "                    \"When users request {context} data, {company} provides it within legal timeframes.\",\n",
    "                    \"Although verification is needed, {company} honors {context} data rights.\",\n",
    "                    \"After receiving a request, {company} promptly provides {context} information.\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return templates.get(category_id, {})\n",
    "    \n",
    "    def generate_template_hash(self, text: str) -> str:\n",
    "        \"\"\"Generate hash for template deduplication\"\"\"\n",
    "        # Normalize text: lowercase, remove extra spaces\n",
    "        normalized = re.sub(r'\\s+', ' ', text.lower().strip())\n",
    "        return hashlib.md5(normalized.encode()).hexdigest()\n",
    "    \n",
    "    def calculate_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate simple word-based similarity\"\"\"\n",
    "        words1 = set(text1.lower().split())\n",
    "        words2 = set(text2.lower().split())\n",
    "        \n",
    "        if not words1 or not words2:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = words1.intersection(words2)\n",
    "        union = words1.union(words2)\n",
    "        \n",
    "        return len(intersection) / len(union) if union else 0.0\n",
    "    \n",
    "    def is_unique_template(self, text: str, similarity_threshold: float = 0.85) -> bool:\n",
    "        \"\"\"Check if template is unique enough\"\"\"\n",
    "        # Check hash for exact duplicates\n",
    "        text_hash = self.generate_template_hash(text)\n",
    "        if text_hash in self.template_hashes:\n",
    "            return False\n",
    "        \n",
    "        # Check similarity with existing templates\n",
    "        for existing_template in self.generated_templates:\n",
    "            similarity = self.calculate_similarity(text, existing_template)\n",
    "            if similarity > similarity_threshold:\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def generate_sample(self, category_id: int, region: str, context_type: str) -> Dict:\n",
    "        \"\"\"Generate a single training sample\"\"\"\n",
    "        # Get templates for this category\n",
    "        category_templates = self.get_category_templates(category_id)\n",
    "        \n",
    "        # Randomly select structure type and formality\n",
    "        structure = random.choice(self.structure_types)\n",
    "        formality = random.choice(self.formality_levels)\n",
    "        \n",
    "        # Get template pattern\n",
    "        template_pattern = random.choice(category_templates.get(structure, category_templates['simple']))\n",
    "        \n",
    "        # Get company and context\n",
    "        company = random.choice(self.companies[region])\n",
    "        context = random.choice(self.contexts[context_type])\n",
    "        modal = random.choice(self.modals)\n",
    "        \n",
    "        # Generate text\n",
    "        text = template_pattern.format(\n",
    "            company=company,\n",
    "            context=context,\n",
    "            modal=modal\n",
    "        )\n",
    "        \n",
    "        # Create sample dictionary\n",
    "        sample = {\n",
    "            'text': text,\n",
    "            'label': category_id,\n",
    "            'template_id': self.generate_template_hash(text),\n",
    "            'language': 'en',\n",
    "            'metadata': {\n",
    "                'company': company,\n",
    "                'context_type': context_type,\n",
    "                'context': context,\n",
    "                'region': region,\n",
    "                'structure': structure,\n",
    "                'formality': formality,\n",
    "                'category_name': PDPL_CATEGORIES[category_id]['en']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def generate_diverse_samples(self, category_id: int, count: int) -> List[Dict]:\n",
    "        \"\"\"Generate diverse samples for a category\"\"\"\n",
    "        samples = []\n",
    "        attempts = 0\n",
    "        max_attempts = count * 10  # Allow 10x attempts for uniqueness\n",
    "        \n",
    "        regions = list(self.companies.keys())\n",
    "        context_types = list(self.contexts.keys())\n",
    "        \n",
    "        while len(samples) < count and attempts < max_attempts:\n",
    "            # Randomly select region and context type\n",
    "            region = random.choice(regions)\n",
    "            context_type = random.choice(context_types)\n",
    "            \n",
    "            # Generate sample\n",
    "            sample = self.generate_sample(category_id, region, context_type)\n",
    "            \n",
    "            # Check uniqueness\n",
    "            if self.is_unique_template(sample['text']):\n",
    "                samples.append(sample)\n",
    "                self.generated_templates.add(sample['text'])\n",
    "                self.template_hashes.add(sample['template_id'])\n",
    "            \n",
    "            attempts += 1\n",
    "        \n",
    "        if len(samples) < count:\n",
    "            print(f\"  Warning: Only generated {len(samples)}/{count} unique samples for category {category_id}\", flush=True)\n",
    "        \n",
    "        return samples\n",
    "\n",
    "# Initialize generator\n",
    "print(\"Initializing English template generator...\", flush=True)\n",
    "generator = EnglishTemplateGenerator()\n",
    "\n",
    "print(f\"Companies available: {sum(len(v) for v in ENGLISH_COMPANIES.values())}\", flush=True)\n",
    "print(f\"Business contexts: {sum(len(v) for v in BUSINESS_CONTEXTS_EN.values())}\", flush=True)\n",
    "print(f\"Template structures: {len(generator.structure_types)}\", flush=True)\n",
    "print(f\"Formality levels: {len(generator.formality_levels)}\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "print(\"Template generator ready!\\n\", flush=True)\n",
    "print(\"=\"*70, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824dce42",
   "metadata": {},
   "source": [
    "## Step 5: Generate English Dataset\n",
    "\n",
    "**Generate 5,000 unique English PDPL templates (625 per category)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0411c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70, flush=True)\n",
    "print(\"STEP 5: GENERATE ENGLISH DATASET\", flush=True)\n",
    "print(\"=\"*70 + \"\\n\", flush=True)\n",
    "\n",
    "# Generate samples for all categories\n",
    "all_samples = []\n",
    "category_stats = {}\n",
    "\n",
    "print(f\"Generating {DATASET_TARGET:,} English PDPL templates...\\n\", flush=True)\n",
    "\n",
    "for category_id in range(NUM_CATEGORIES):\n",
    "    category_name = PDPL_CATEGORIES[category_id]['en']\n",
    "    print(f\"Category {category_id}: {category_name}\", flush=True)\n",
    "    print(f\"  Target: {SAMPLES_PER_CATEGORY} samples\", flush=True)\n",
    "    \n",
    "    # Generate samples\n",
    "    samples = generator.generate_diverse_samples(category_id, SAMPLES_PER_CATEGORY)\n",
    "    \n",
    "    # Add to all samples\n",
    "    all_samples.extend(samples)\n",
    "    \n",
    "    # Track statistics\n",
    "    category_stats[category_id] = {\n",
    "        'count': len(samples),\n",
    "        'name': category_name,\n",
    "        'structures': Counter([s['metadata']['structure'] for s in samples]),\n",
    "        'formality': Counter([s['metadata']['formality'] for s in samples]),\n",
    "        'regions': Counter([s['metadata']['region'] for s in samples])\n",
    "    }\n",
    "    \n",
    "    print(f\"  Generated: {len(samples)} samples\", flush=True)\n",
    "    print(f\"  Structures: {dict(category_stats[category_id]['structures'])}\", flush=True)\n",
    "    print(flush=True)\n",
    "\n",
    "# Shuffle all samples\n",
    "random.shuffle(all_samples)\n",
    "\n",
    "print(\"=\"*70, flush=True)\n",
    "print(\"DATASET GENERATION COMPLETE\", flush=True)\n",
    "print(\"=\"*70 + \"\\n\", flush=True)\n",
    "\n",
    "print(f\"Total samples generated: {len(all_samples):,}\", flush=True)\n",
    "print(f\"Target samples: {DATASET_TARGET:,}\", flush=True)\n",
    "print(f\"Unique templates: {len(generator.generated_templates):,}\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "# Category distribution\n",
    "print(\"Category Distribution:\", flush=True)\n",
    "for cat_id, stats in category_stats.items():\n",
    "    percentage = (stats['count'] / len(all_samples)) * 100\n",
    "    print(f\"  Cat {cat_id} ({stats['name']}): {stats['count']} ({percentage:.1f}%)\", flush=True)\n",
    "\n",
    "print(flush=True)\n",
    "\n",
    "# Save complete dataset\n",
    "print(f\"Saving dataset to {OUTPUT_DATASET_FILE}...\", flush=True)\n",
    "with open(OUTPUT_DATASET_FILE, 'w', encoding='utf-8') as f:\n",
    "    for sample in all_samples:\n",
    "        f.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Dataset saved successfully!\\n\", flush=True)\n",
    "print(\"=\"*70, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4adf70",
   "metadata": {},
   "source": [
    "## Step 6: Reserved Company Sets for Data Isolation\n",
    "\n",
    "**Critical: Prevent data leakage by isolating companies across train/val/test splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a183510",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70, flush=True)\n",
    "print(\"STEP 6: RESERVED COMPANY SETS (DATA LEAK PREVENTION)\", flush=True)\n",
    "print(\"=\"*70 + \"\\n\", flush=True)\n",
    "\n",
    "# Reserve companies for each split to prevent data leakage\n",
    "# This ensures NO company appears in multiple splits\n",
    "RESERVED_COMPANIES = {\n",
    "    'train': {\n",
    "        'north': ['VNG', 'FPT', 'VNPT', 'Viettel', 'Vingroup', 'MB Bank', 'ACB', 'VPBank'],\n",
    "        'central': ['Vinamilk', 'Hoa Phat', 'Petrolimex', 'PVN', 'EVN'],\n",
    "        'south': ['Shopee VN', 'Lazada VN', 'Tiki', 'Grab VN', 'MoMo', 'ZaloPay', 'Techcombank']\n",
    "    },\n",
    "    'val': {\n",
    "        'north': ['VietinBank', 'Agribank'],\n",
    "        'central': ['Vinatex', 'DHG Pharma'],\n",
    "        'south': ['VCB', 'Sacombank']\n",
    "    },\n",
    "    'test': {\n",
    "        'north': ['BIDV', 'TPBank'],\n",
    "        'central': ['Hau Giang Pharma'],\n",
    "        'south': ['HDBank']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Verify no company overlap between splits\n",
    "print(\"Verifying company isolation...\\n\", flush=True)\n",
    "\n",
    "all_train_companies = []\n",
    "all_val_companies = []\n",
    "all_test_companies = []\n",
    "\n",
    "for region in ['north', 'central', 'south']:\n",
    "    all_train_companies.extend(RESERVED_COMPANIES['train'][region])\n",
    "    all_val_companies.extend(RESERVED_COMPANIES['val'][region])\n",
    "    all_test_companies.extend(RESERVED_COMPANIES['test'][region])\n",
    "\n",
    "# Check for overlaps\n",
    "train_val_overlap = set(all_train_companies).intersection(set(all_val_companies))\n",
    "train_test_overlap = set(all_train_companies).intersection(set(all_test_companies))\n",
    "val_test_overlap = set(all_val_companies).intersection(set(all_test_companies))\n",
    "\n",
    "print(f\"Train companies: {len(all_train_companies)}\", flush=True)\n",
    "print(f\"Val companies: {len(all_val_companies)}\", flush=True)\n",
    "print(f\"Test companies: {len(all_test_companies)}\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "print(f\"Train-Val overlap: {len(train_val_overlap)} (should be 0)\", flush=True)\n",
    "print(f\"Train-Test overlap: {len(train_test_overlap)} (should be 0)\", flush=True)\n",
    "print(f\"Val-Test overlap: {len(val_test_overlap)} (should be 0)\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "if len(train_val_overlap) == 0 and len(train_test_overlap) == 0 and len(val_test_overlap) == 0:\n",
    "    print(\"SUCCESS: Complete company isolation achieved!\", flush=True)\n",
    "    print(\"No data leakage possible from company overlap.\\n\", flush=True)\n",
    "else:\n",
    "    print(\"WARNING: Company overlap detected!\", flush=True)\n",
    "    if train_val_overlap:\n",
    "        print(f\"  Train-Val: {train_val_overlap}\", flush=True)\n",
    "    if train_test_overlap:\n",
    "        print(f\"  Train-Test: {train_test_overlap}\", flush=True)\n",
    "    if val_test_overlap:\n",
    "        print(f\"  Val-Test: {val_test_overlap}\", flush=True)\n",
    "    print(flush=True)\n",
    "\n",
    "print(\"=\"*70, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a7d37",
   "metadata": {},
   "source": [
    "## Step 7: Data Splitting with Company Isolation\n",
    "\n",
    "**Split dataset into Train/Val/Test with reserved company sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b79e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70, flush=True)\n",
    "print(\"STEP 7: DATA SPLITTING WITH COMPANY ISOLATION\", flush=True)\n",
    "print(\"=\"*70 + \"\\n\", flush=True)\n",
    "\n",
    "# Split samples by reserved company sets\n",
    "train_samples = []\n",
    "val_samples = []\n",
    "test_samples = []\n",
    "\n",
    "print(\"Splitting dataset by reserved companies...\\n\", flush=True)\n",
    "\n",
    "for sample in all_samples:\n",
    "    company = sample['metadata']['company']\n",
    "    \n",
    "    # Check which split this company belongs to\n",
    "    if company in all_train_companies:\n",
    "        train_samples.append(sample)\n",
    "    elif company in all_val_companies:\n",
    "        val_samples.append(sample)\n",
    "    elif company in all_test_companies:\n",
    "        test_samples.append(sample)\n",
    "    else:\n",
    "        # Fallback: assign to train if company not in reserved sets\n",
    "        train_samples.append(sample)\n",
    "\n",
    "# Shuffle each split\n",
    "random.shuffle(train_samples)\n",
    "random.shuffle(val_samples)\n",
    "random.shuffle(test_samples)\n",
    "\n",
    "print(\"Split Summary:\", flush=True)\n",
    "print(f\"  Train: {len(train_samples):,} samples ({len(train_samples)/len(all_samples)*100:.1f}%)\", flush=True)\n",
    "print(f\"  Val: {len(val_samples):,} samples ({len(val_samples)/len(all_samples)*100:.1f}%)\", flush=True)\n",
    "print(f\"  Test: {len(test_samples):,} samples ({len(test_samples)/len(all_samples)*100:.1f}%)\", flush=True)\n",
    "print(f\"  Total: {len(train_samples) + len(val_samples) + len(test_samples):,} samples\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "# Verify category distribution in each split\n",
    "print(\"Category Distribution by Split:\", flush=True)\n",
    "for split_name, split_samples in [('Train', train_samples), ('Val', val_samples), ('Test', test_samples)]:\n",
    "    category_counts = Counter([s['label'] for s in split_samples])\n",
    "    print(f\"\\n  {split_name}:\", flush=True)\n",
    "    for cat_id in range(NUM_CATEGORIES):\n",
    "        count = category_counts.get(cat_id, 0)\n",
    "        percentage = (count / len(split_samples) * 100) if split_samples else 0\n",
    "        print(f\"    Cat {cat_id}: {count} ({percentage:.1f}%)\", flush=True)\n",
    "\n",
    "print(flush=True)\n",
    "\n",
    "# Save split datasets\n",
    "print(\"Saving split datasets...\", flush=True)\n",
    "\n",
    "with open(TRAIN_FILE, 'w', encoding='utf-8') as f:\n",
    "    for sample in train_samples:\n",
    "        f.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "print(f\"  {TRAIN_FILE}: {len(train_samples):,} samples\", flush=True)\n",
    "\n",
    "with open(VAL_FILE, 'w', encoding='utf-8') as f:\n",
    "    for sample in val_samples:\n",
    "        f.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "print(f\"  {VAL_FILE}: {len(val_samples):,} samples\", flush=True)\n",
    "\n",
    "with open(TEST_FILE, 'w', encoding='utf-8') as f:\n",
    "    for sample in test_samples:\n",
    "        f.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "print(f\"  {TEST_FILE}: {len(test_samples):,} samples\", flush=True)\n",
    "\n",
    "print(flush=True)\n",
    "print(\"Data splitting complete!\\n\", flush=True)\n",
    "print(\"=\"*70, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74f7a7a",
   "metadata": {},
   "source": [
    "## Step 8: Data Leakage Detection & Diagnostics\n",
    "\n",
    "**CRITICAL: Comprehensive data leakage analysis to ensure model validity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc212832",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70, flush=True)\n",
    "print(\"STEP 8: DATA LEAKAGE DETECTION & DIAGNOSTICS\", flush=True)\n",
    "print(\"=\"*70 + \"\\n\", flush=True)\n",
    "\n",
    "# CRITICAL DATA LEAKAGE CHECKS\n",
    "# This section ensures NO information leaks between train/val/test splits\n",
    "\n",
    "print(\"CRITICAL DATA INTEGRITY CHECKS\\n\", flush=True)\n",
    "print(\"-\" * 70, flush=True)\n",
    "\n",
    "# CHECK 1: Template Overlap Detection\n",
    "print(\"\\nCHECK 1: Template Overlap Detection\", flush=True)\n",
    "print(\"-\" * 70, flush=True)\n",
    "\n",
    "train_texts = set([s['text'] for s in train_samples])\n",
    "val_texts = set([s['text'] for s in val_samples])\n",
    "test_texts = set([s['text'] for s in test_samples])\n",
    "\n",
    "train_val_text_overlap = train_texts.intersection(val_texts)\n",
    "train_test_text_overlap = train_texts.intersection(test_texts)\n",
    "val_test_text_overlap = val_texts.intersection(test_texts)\n",
    "\n",
    "print(f\"Train samples: {len(train_texts):,}\", flush=True)\n",
    "print(f\"Val samples: {len(val_texts):,}\", flush=True)\n",
    "print(f\"Test samples: {len(test_texts):,}\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "print(f\"Train-Val template overlap: {len(train_val_text_overlap)} (MUST be 0)\", flush=True)\n",
    "print(f\"Train-Test template overlap: {len(train_test_text_overlap)} (MUST be 0)\", flush=True)\n",
    "print(f\"Val-Test template overlap: {len(val_test_text_overlap)} (MUST be 0)\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "template_overlap_ok = (len(train_val_text_overlap) == 0 and \n",
    "                       len(train_test_text_overlap) == 0 and \n",
    "                       len(val_test_text_overlap) == 0)\n",
    "\n",
    "if template_overlap_ok:\n",
    "    print(\"SUCCESS: ZERO template overlap detected!\", flush=True)\n",
    "    print(\"Data splits are completely isolated.\\n\", flush=True)\n",
    "else:\n",
    "    print(\"CRITICAL WARNING: Template overlap detected!\", flush=True)\n",
    "    if train_val_text_overlap:\n",
    "        print(f\"  Train-Val overlap: {list(train_val_text_overlap)[:3]}\", flush=True)\n",
    "    if train_test_text_overlap:\n",
    "        print(f\"  Train-Test overlap: {list(train_test_text_overlap)[:3]}\", flush=True)\n",
    "    if val_test_text_overlap:\n",
    "        print(f\"  Val-Test overlap: {list(val_test_text_overlap)[:3]}\", flush=True)\n",
    "    print(flush=True)\n",
    "\n",
    "# CHECK 2: Company Overlap Detection\n",
    "print(\"\\nCHECK 2: Company Overlap Detection\", flush=True)\n",
    "print(\"-\" * 70, flush=True)\n",
    "\n",
    "train_companies = set([s['metadata']['company'] for s in train_samples])\n",
    "val_companies = set([s['metadata']['company'] for s in val_samples])\n",
    "test_companies = set([s['metadata']['company'] for s in test_samples])\n",
    "\n",
    "train_val_company_overlap = train_companies.intersection(val_companies)\n",
    "train_test_company_overlap = train_companies.intersection(test_companies)\n",
    "val_test_company_overlap = val_companies.intersection(test_companies)\n",
    "\n",
    "print(f\"Train companies: {len(train_companies)}\", flush=True)\n",
    "print(f\"Val companies: {len(val_companies)}\", flush=True)\n",
    "print(f\"Test companies: {len(test_companies)}\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "print(f\"Train-Val company overlap: {len(train_val_company_overlap)} (MUST be 0)\", flush=True)\n",
    "print(f\"Train-Test company overlap: {len(train_test_company_overlap)} (MUST be 0)\", flush=True)\n",
    "print(f\"Val-Test company overlap: {len(val_test_company_overlap)} (MUST be 0)\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "company_overlap_ok = (len(train_val_company_overlap) == 0 and \n",
    "                      len(train_test_company_overlap) == 0 and \n",
    "                      len(val_test_company_overlap) == 0)\n",
    "\n",
    "if company_overlap_ok:\n",
    "    print(\"SUCCESS: ZERO company overlap detected!\", flush=True)\n",
    "    print(\"Companies are completely isolated across splits.\\n\", flush=True)\n",
    "else:\n",
    "    print(\"CRITICAL WARNING: Company overlap detected!\", flush=True)\n",
    "    if train_val_company_overlap:\n",
    "        print(f\"  Train-Val: {train_val_company_overlap}\", flush=True)\n",
    "    if train_test_company_overlap:\n",
    "        print(f\"  Train-Test: {train_test_company_overlap}\", flush=True)\n",
    "    if val_test_company_overlap:\n",
    "        print(f\"  Val-Test: {val_test_company_overlap}\", flush=True)\n",
    "    print(flush=True)\n",
    "\n",
    "# CHECK 3: Template Hash Overlap (Double verification)\n",
    "print(\"\\nCHECK 3: Template Hash Overlap (Double Verification)\", flush=True)\n",
    "print(\"-\" * 70, flush=True)\n",
    "\n",
    "train_hashes = set([s['template_id'] for s in train_samples])\n",
    "val_hashes = set([s['template_id'] for s in val_samples])\n",
    "test_hashes = set([s['template_id'] for s in test_samples])\n",
    "\n",
    "train_val_hash_overlap = train_hashes.intersection(val_hashes)\n",
    "train_test_hash_overlap = train_hashes.intersection(test_hashes)\n",
    "val_test_hash_overlap = val_hashes.intersection(test_hashes)\n",
    "\n",
    "print(f\"Train-Val hash overlap: {len(train_val_hash_overlap)} (MUST be 0)\", flush=True)\n",
    "print(f\"Train-Test hash overlap: {len(train_test_hash_overlap)} (MUST be 0)\", flush=True)\n",
    "print(f\"Val-Test hash overlap: {len(val_test_hash_overlap)} (MUST be 0)\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "hash_overlap_ok = (len(train_val_hash_overlap) == 0 and \n",
    "                   len(train_test_hash_overlap) == 0 and \n",
    "                   len(val_test_hash_overlap) == 0)\n",
    "\n",
    "if hash_overlap_ok:\n",
    "    print(\"SUCCESS: ZERO hash overlap detected!\", flush=True)\n",
    "    print(\"Template uniqueness verified.\\n\", flush=True)\n",
    "else:\n",
    "    print(\"CRITICAL WARNING: Hash overlap detected!\", flush=True)\n",
    "    print(f\"  This indicates duplicate templates exist.\\n\", flush=True)\n",
    "\n",
    "# CHECK 4: Similarity Analysis (Sample-based)\n",
    "print(\"\\nCHECK 4: Cross-Split Similarity Analysis\", flush=True)\n",
    "print(\"-\" * 70, flush=True)\n",
    "\n",
    "def calculate_text_similarity(text1, text2):\n",
    "    \"\"\"Calculate word-based similarity\"\"\"\n",
    "    words1 = set(text1.lower().split())\n",
    "    words2 = set(text2.lower().split())\n",
    "    if not words1 or not words2:\n",
    "        return 0.0\n",
    "    intersection = words1.intersection(words2)\n",
    "    union = words1.union(words2)\n",
    "    return len(intersection) / len(union) if union else 0.0\n",
    "\n",
    "# Sample 100 random pairs from each split combination\n",
    "sample_size = min(100, len(val_samples), len(test_samples))\n",
    "\n",
    "print(f\"Analyzing {sample_size} random sample pairs...\\n\", flush=True)\n",
    "\n",
    "# Train-Val similarity\n",
    "train_val_similarities = []\n",
    "for _ in range(sample_size):\n",
    "    train_sample = random.choice(train_samples)\n",
    "    val_sample = random.choice(val_samples)\n",
    "    sim = calculate_text_similarity(train_sample['text'], val_sample['text'])\n",
    "    train_val_similarities.append(sim)\n",
    "\n",
    "# Train-Test similarity  \n",
    "train_test_similarities = []\n",
    "for _ in range(sample_size):\n",
    "    train_sample = random.choice(train_samples)\n",
    "    test_sample = random.choice(test_samples)\n",
    "    sim = calculate_text_similarity(train_sample['text'], test_sample['text'])\n",
    "    train_test_similarities.append(sim)\n",
    "\n",
    "# Val-Test similarity\n",
    "val_test_similarities = []\n",
    "for _ in range(sample_size):\n",
    "    val_sample = random.choice(val_samples)\n",
    "    test_sample = random.choice(test_samples)\n",
    "    sim = calculate_text_similarity(val_sample['text'], test_sample['text'])\n",
    "    val_test_similarities.append(sim)\n",
    "\n",
    "avg_train_val_sim = np.mean(train_val_similarities)\n",
    "avg_train_test_sim = np.mean(train_test_similarities)\n",
    "avg_val_test_sim = np.mean(val_test_similarities)\n",
    "\n",
    "max_train_val_sim = np.max(train_val_similarities)\n",
    "max_train_test_sim = np.max(train_test_similarities)\n",
    "max_val_test_sim = np.max(val_test_similarities)\n",
    "\n",
    "print(f\"Train-Val similarity:\", flush=True)\n",
    "print(f\"  Average: {avg_train_val_sim:.4f}\", flush=True)\n",
    "print(f\"  Maximum: {max_train_val_sim:.4f}\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "print(f\"Train-Test similarity:\", flush=True)\n",
    "print(f\"  Average: {avg_train_test_sim:.4f}\", flush=True)\n",
    "print(f\"  Maximum: {max_train_test_sim:.4f}\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "print(f\"Val-Test similarity:\", flush=True)\n",
    "print(f\"  Average: {avg_val_test_sim:.4f}\", flush=True)\n",
    "print(f\"  Maximum: {max_val_test_sim:.4f}\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "SIMILARITY_THRESHOLD = 0.85\n",
    "high_similarity_detected = (max_train_val_sim > SIMILARITY_THRESHOLD or \n",
    "                            max_train_test_sim > SIMILARITY_THRESHOLD or \n",
    "                            max_val_test_sim > SIMILARITY_THRESHOLD)\n",
    "\n",
    "if not high_similarity_detected:\n",
    "    print(f\"SUCCESS: All similarities below threshold ({SIMILARITY_THRESHOLD})\", flush=True)\n",
    "    print(\"Templates are sufficiently diverse.\\n\", flush=True)\n",
    "else:\n",
    "    print(f\"WARNING: High similarity detected (>{SIMILARITY_THRESHOLD})\", flush=True)\n",
    "    print(\"Review template diversity.\\n\", flush=True)\n",
    "\n",
    "# FINAL VERDICT\n",
    "print(\"=\" * 70, flush=True)\n",
    "print(\"DATA LEAKAGE DETECTION - FINAL VERDICT\", flush=True)\n",
    "print(\"=\" * 70 + \"\\n\", flush=True)\n",
    "\n",
    "all_checks_passed = (template_overlap_ok and \n",
    "                     company_overlap_ok and \n",
    "                     hash_overlap_ok and \n",
    "                     not high_similarity_detected)\n",
    "\n",
    "if all_checks_passed:\n",
    "    print(\"SUCCESS: ALL DATA INTEGRITY CHECKS PASSED!\", flush=True)\n",
    "    print(flush=True)\n",
    "    print(\"Data leakage prevention verified:\", flush=True)\n",
    "    print(\"  - ZERO template overlap\", flush=True)\n",
    "    print(\"  - ZERO company overlap\", flush=True)\n",
    "    print(\"  - ZERO hash overlap\", flush=True)\n",
    "    print(f\"  - Low cross-split similarity (<{SIMILARITY_THRESHOLD})\", flush=True)\n",
    "    print(flush=True)\n",
    "    print(\"The dataset is READY for training.\", flush=True)\n",
    "    print(\"Model evaluation will be VALID and UNBIASED.\\n\", flush=True)\n",
    "else:\n",
    "    print(\"WARNING: Some integrity checks failed!\", flush=True)\n",
    "    print(\"Review the warnings above before proceeding.\\n\", flush=True)\n",
    "\n",
    "print(\"=\" * 70, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caeb135",
   "metadata": {},
   "source": [
    "## Step 9: Load Datasets from Files\n",
    "\n",
    "Load the train/val/test datasets from JSONL files for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3818e927",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70, flush=True)\n",
    "print(\"STEP 9: LOADING DATASETS FROM FILES\", flush=True)\n",
    "print(\"=\"*70 + \"\\n\", flush=True)\n",
    "\n",
    "# Load datasets from saved JSONL files (using variables from Step 2)\n",
    "train_file = TRAIN_FILE\n",
    "val_file = VAL_FILE\n",
    "test_file = TEST_FILE\n",
    "\n",
    "print(\"Loading datasets from:\", flush=True)\n",
    "print(f\"  Train: {train_file}\", flush=True)\n",
    "print(f\"  Val: {val_file}\", flush=True)\n",
    "print(f\"  Test: {test_file}\\n\", flush=True)\n",
    "\n",
    "# Load train dataset\n",
    "with open(train_file, 'r', encoding='utf-8') as f:\n",
    "    train_data = [json.loads(line.strip()) for line in f]\n",
    "\n",
    "# Load validation dataset\n",
    "with open(val_file, 'r', encoding='utf-8') as f:\n",
    "    val_data = [json.loads(line.strip()) for line in f]\n",
    "\n",
    "# Load test dataset\n",
    "with open(test_file, 'r', encoding='utf-8') as f:\n",
    "    test_data = [json.loads(line.strip()) for line in f]\n",
    "\n",
    "print(\"Dataset Sizes:\", flush=True)\n",
    "print(f\"  Train: {len(train_data):,} samples\", flush=True)\n",
    "print(f\"  Val: {len(val_data):,} samples\", flush=True)\n",
    "print(f\"  Test: {len(test_data):,} samples\", flush=True)\n",
    "print(f\"  Total: {len(train_data) + len(val_data) + len(test_data):,} samples\\n\", flush=True)\n",
    "\n",
    "# Verify category distribution (samples use 'label' key, not 'category')\n",
    "print(\"Category Distribution (Train):\", flush=True)\n",
    "train_categories = {}\n",
    "for sample in train_data:\n",
    "    cat = sample['label']  # Changed from 'category' to 'label'\n",
    "    train_categories[cat] = train_categories.get(cat, 0) + 1\n",
    "\n",
    "for cat_id in sorted(train_categories.keys()):\n",
    "    count = train_categories[cat_id]\n",
    "    percentage = (count / len(train_data)) * 100\n",
    "    cat_name = PDPL_CATEGORIES[cat_id]['en']\n",
    "    print(f\"  Category {cat_id} ({cat_name}): {count:,} ({percentage:.1f}%)\", flush=True)\n",
    "\n",
    "print(\"\\nDatasets loaded successfully!\", flush=True)\n",
    "print(\"=\" * 70, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f2d484",
   "metadata": {},
   "source": [
    "## Step 10: BERT Tokenizer Setup\n",
    "\n",
    "Initialize BERT-base-uncased tokenizer for English text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f804615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70, flush=True)\n",
    "print(\"STEP 10: BERT TOKENIZER SETUP\", flush=True)\n",
    "print(\"=\"*70 + \"\\n\", flush=True)\n",
    "\n",
    "# Load BERT tokenizer (using MODEL_NAME from Step 2)\n",
    "print(f\"Loading tokenizer: {MODEL_NAME}\", flush=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"\\nTokenizer loaded successfully!\", flush=True)\n",
    "print(f\"  Model: {MODEL_NAME}\", flush=True)\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size:,}\", flush=True)\n",
    "print(f\"  Max model length: {tokenizer.model_max_length}\", flush=True)\n",
    "print(f\"  Case: uncased (lowercase)\", flush=True)\n",
    "print(f\"  Tokenization: WordPiece\\n\", flush=True)\n",
    "\n",
    "# Test tokenizer with sample text\n",
    "sample_text = \"VNG Corporation must process customer data lawfully under PDPL 2025.\"\n",
    "print(\"Tokenizer Test:\", flush=True)\n",
    "print(f\"  Text: {sample_text}\", flush=True)\n",
    "\n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "print(f\"  Tokens: {tokens[:10]}... ({len(tokens)} total)\", flush=True)\n",
    "\n",
    "encoded = tokenizer.encode(sample_text, add_special_tokens=True)\n",
    "print(f\"  Token IDs: {encoded[:10]}... ({len(encoded)} total)\", flush=True)\n",
    "\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"  Decoded: {decoded}\\n\", flush=True)\n",
    "\n",
    "# Tokenize function for dataset\n",
    "MAX_LENGTH = 256  # Standard for BERT\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize text samples with padding and truncation\"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "print(\"Tokenization function defined.\", flush=True)\n",
    "print(f\"Max sequence length: {MAX_LENGTH} tokens\", flush=True)\n",
    "print(\"Padding: max_length\", flush=True)\n",
    "print(\"Truncation: enabled\\n\", flush=True)\n",
    "\n",
    "print(\"=\" * 70, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d11b9",
   "metadata": {},
   "source": [
    "## Step 11: Model Loading and Preparation\n",
    "\n",
    "Load BERT-base-uncased model and apply English-specific hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10239bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70, flush=True)\n",
    "print(\"STEP 11: MODEL LOADING AND PREPARATION\", flush=True)\n",
    "print(\"=\"*70 + \"\\n\", flush=True)\n",
    "\n",
    "# Load BERT model for sequence classification\n",
    "num_labels = len(PDPL_CATEGORIES)\n",
    "print(f\"Loading model: {MODEL_NAME}\", flush=True)\n",
    "print(f\"Number of labels: {num_labels}\", flush=True)\n",
    "print(f\"Device: {device}\\n\", flush=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\", flush=True)\n",
    "print(f\"  Model class: {type(model).__name__}\", flush=True)\n",
    "print(f\"  Number of parameters: {model.num_parameters():,}\", flush=True)\n",
    "print(f\"  Number of labels: {model.config.num_labels}\\n\", flush=True)\n",
    "\n",
    "# Apply English hyperparameters (access directly from TRAINING_CONFIG)\n",
    "print(\"Applying English-specific hyperparameters:\", flush=True)\n",
    "\n",
    "# Set dropout rate\n",
    "model.config.hidden_dropout_prob = TRAINING_CONFIG['hidden_dropout_prob']\n",
    "model.config.attention_probs_dropout_prob = TRAINING_CONFIG['attention_probs_dropout_prob']\n",
    "\n",
    "print(f\"  Hidden dropout: {model.config.hidden_dropout_prob}\", flush=True)\n",
    "print(f\"  Attention dropout: {model.config.attention_probs_dropout_prob}\", flush=True)\n",
    "print(f\"  Learning rate: {TRAINING_CONFIG['learning_rate']}\", flush=True)\n",
    "print(f\"  Label smoothing: {TRAINING_CONFIG['label_smoothing_factor']}\", flush=True)\n",
    "print(f\"  Epochs: {TRAINING_CONFIG['num_train_epochs']}\", flush=True)\n",
    "print(f\"  Batch size: {TRAINING_CONFIG['per_device_train_batch_size']}\\n\", flush=True)\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "print(f\"Model moved to: {device}\", flush=True)\n",
    "\n",
    "# Display model architecture summary\n",
    "print(\"\\nModel Architecture Summary:\", flush=True)\n",
    "print(f\"  Embedding dim: {model.config.hidden_size}\", flush=True)\n",
    "print(f\"  Attention heads: {model.config.num_attention_heads}\", flush=True)\n",
    "print(f\"  Hidden layers: {model.config.num_hidden_layers}\", flush=True)\n",
    "print(f\"  Intermediate size: {model.config.intermediate_size}\", flush=True)\n",
    "print(f\"  Max position embeddings: {model.config.max_position_embeddings}\\n\", flush=True)\n",
    "\n",
    "# Label mapping\n",
    "print(\"Label Mapping:\", flush=True)\n",
    "for cat_id, cat_data in PDPL_CATEGORIES.items():\n",
    "    print(f\"  {cat_id}: {cat_data['en']}\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddf9fa6",
   "metadata": {},
   "source": [
    "## Step 12: Prepare Dataset Objects for Training\n",
    "\n",
    "Create HuggingFace Dataset objects and apply tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ac5662",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70, flush=True)\n",
    "print(\"STEP 12: PREPARE DATASET OBJECTS FOR TRAINING\", flush=True)\n",
    "print(\"=\"*70 + \"\\n\", flush=True)\n",
    "\n",
    "# Convert to HuggingFace Dataset format\n",
    "from datasets import Dataset\n",
    "\n",
    "print(\"Converting data to HuggingFace Dataset format...\\n\", flush=True)\n",
    "\n",
    "# Extract text and labels (using 'label' key, not 'category')\n",
    "train_texts = [sample['text'] for sample in train_data]\n",
    "train_labels = [sample['label'] for sample in train_data]\n",
    "\n",
    "val_texts = [sample['text'] for sample in val_data]\n",
    "val_labels = [sample['label'] for sample in val_data]\n",
    "\n",
    "test_texts = [sample['text'] for sample in test_data]\n",
    "test_labels = [sample['label'] for sample in test_data]\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'text': train_texts,\n",
    "    'label': train_labels\n",
    "})\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    'text': val_texts,\n",
    "    'label': val_labels\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'text': test_texts,\n",
    "    'label': test_labels\n",
    "})\n",
    "\n",
    "print(\"Datasets created:\", flush=True)\n",
    "print(f\"  Train: {len(train_dataset):,} samples\", flush=True)\n",
    "print(f\"  Val: {len(val_dataset):,} samples\", flush=True)\n",
    "print(f\"  Test: {len(test_dataset):,} samples\\n\", flush=True)\n",
    "\n",
    "# Apply tokenization\n",
    "print(\"Applying tokenization...\", flush=True)\n",
    "\n",
    "def tokenize_batch(examples):\n",
    "    \"\"\"Tokenize a batch of examples\"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_batch, batched=True, batch_size=1000)\n",
    "val_dataset = val_dataset.map(tokenize_batch, batched=True, batch_size=1000)\n",
    "test_dataset = test_dataset.map(tokenize_batch, batched=True, batch_size=1000)\n",
    "\n",
    "print(\"Tokenization complete!\\n\", flush=True)\n",
    "\n",
    "# Set dataset format for PyTorch\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "print(\"Dataset format set to PyTorch tensors.\", flush=True)\n",
    "print(\"\\nDataset Columns:\", flush=True)\n",
    "print(f\"  {train_dataset.column_names}\", flush=True)\n",
    "\n",
    "print(\"\\nSample from train dataset:\", flush=True)\n",
    "sample = train_dataset[0]\n",
    "print(f\"  input_ids shape: {sample['input_ids'].shape}\", flush=True)\n",
    "print(f\"  attention_mask shape: {sample['attention_mask'].shape}\", flush=True)\n",
    "print(f\"  label: {sample['label']}\", flush=True)\n",
    "\n",
    "print(\"\\nDatasets are ready for training!\", flush=True)\n",
    "print(\"=\" * 70, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381271ef",
   "metadata": {},
   "source": [
    "## Step 13: Smart Training Callback for English Model\n",
    "\n",
    "Implement intelligent training monitoring with English-specific thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc36b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70, flush=True)\n",
    "print(\"STEP 13: SMART TRAINING CALLBACK FOR ENGLISH MODEL\", flush=True)\n",
    "print(\"=\"*70 + \"\\n\", flush=True)\n",
    "\n",
    "class SmartTrainingCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Smart callback for English PDPL model training with:\n",
    "    - Early high accuracy detection (0.90 threshold)\n",
    "    - Extreme overfitting prevention (0.95 threshold)\n",
    "    - Training metrics monitoring\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, early_high_accuracy_threshold=0.90, extreme_overfitting_threshold=0.95):\n",
    "        self.early_high_accuracy_threshold = early_high_accuracy_threshold\n",
    "        self.extreme_overfitting_threshold = extreme_overfitting_threshold\n",
    "        self.best_eval_accuracy = 0\n",
    "        self.best_epoch = 0\n",
    "        self.training_history = []\n",
    "        \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"Called at the end of each epoch\"\"\"\n",
    "        \n",
    "        # Get latest metrics\n",
    "        if state.log_history:\n",
    "            latest_logs = state.log_history[-1]\n",
    "            epoch = latest_logs.get('epoch', 0)\n",
    "            \n",
    "            # Extract metrics\n",
    "            train_loss = latest_logs.get('loss', None)\n",
    "            eval_loss = latest_logs.get('eval_loss', None)\n",
    "            eval_accuracy = latest_logs.get('eval_accuracy', None)\n",
    "            \n",
    "            # Store in history\n",
    "            self.training_history.append({\n",
    "                'epoch': epoch,\n",
    "                'train_loss': train_loss,\n",
    "                'eval_loss': eval_loss,\n",
    "                'eval_accuracy': eval_accuracy\n",
    "            })\n",
    "            \n",
    "            if eval_accuracy is not None:\n",
    "                # Update best accuracy\n",
    "                if eval_accuracy > self.best_eval_accuracy:\n",
    "                    self.best_eval_accuracy = eval_accuracy\n",
    "                    self.best_epoch = epoch\n",
    "                \n",
    "                print(f\"\\nEpoch {epoch:.1f} Summary:\", flush=True)\n",
    "                if train_loss:\n",
    "                    print(f\"  Train Loss: {train_loss:.4f}\", flush=True)\n",
    "                if eval_loss:\n",
    "                    print(f\"  Eval Loss: {eval_loss:.4f}\", flush=True)\n",
    "                print(f\"  Eval Accuracy: {eval_accuracy:.4f} ({eval_accuracy*100:.2f}%)\", flush=True)\n",
    "                print(f\"  Best Accuracy: {self.best_eval_accuracy:.4f} (Epoch {self.best_epoch:.1f})\", flush=True)\n",
    "                \n",
    "                # Check for early high accuracy (English target: 88-92%)\n",
    "                if eval_accuracy >= self.early_high_accuracy_threshold:\n",
    "                    print(f\"\\nEARLY HIGH ACCURACY DETECTED!\", flush=True)\n",
    "                    print(f\"  Accuracy {eval_accuracy:.4f} >= threshold {self.early_high_accuracy_threshold}\", flush=True)\n",
    "                    print(f\"  Target range for English: 0.88-0.92 (88-92%)\", flush=True)\n",
    "                    \n",
    "                    # Check if in target range\n",
    "                    if 0.88 <= eval_accuracy <= 0.92:\n",
    "                        print(f\"  PERFECT! Within target range. Consider stopping.\", flush=True)\n",
    "                        control.should_training_stop = True\n",
    "                    elif eval_accuracy > 0.92:\n",
    "                        print(f\"  WARNING: Above target (may indicate overfitting)\", flush=True)\n",
    "                \n",
    "                # Check for extreme overfitting\n",
    "                if eval_accuracy >= self.extreme_overfitting_threshold:\n",
    "                    print(f\"\\nEXTREME OVERFITTING WARNING!\", flush=True)\n",
    "                    print(f\"  Accuracy {eval_accuracy:.4f} >= {self.extreme_overfitting_threshold}\", flush=True)\n",
    "                    print(f\"  Model may not generalize well to unseen data.\", flush=True)\n",
    "                    print(f\"  Stopping training to prevent overfitting...\", flush=True)\n",
    "                    control.should_training_stop = True\n",
    "                \n",
    "                # Check for overfitting (train loss < eval loss significantly)\n",
    "                if train_loss and eval_loss and train_loss < eval_loss * 0.5:\n",
    "                    print(f\"\\n  WARNING: Potential overfitting detected\", flush=True)\n",
    "                    print(f\"  Train loss ({train_loss:.4f}) << Eval loss ({eval_loss:.4f})\", flush=True)\n",
    "        \n",
    "        return control\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"Called at the end of training\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70, flush=True)\n",
    "        print(\"TRAINING COMPLETE - FINAL SUMMARY\", flush=True)\n",
    "        print(\"=\"*70, flush=True)\n",
    "        print(f\"\\nBest Accuracy: {self.best_eval_accuracy:.4f} ({self.best_eval_accuracy*100:.2f}%)\", flush=True)\n",
    "        print(f\"Best Epoch: {self.best_epoch:.1f}\", flush=True)\n",
    "        print(f\"Total Epochs: {len(self.training_history)}\", flush=True)\n",
    "        \n",
    "        # Target assessment\n",
    "        print(f\"\\nEnglish Model Target: 88-92% accuracy\", flush=True)\n",
    "        if 0.88 <= self.best_eval_accuracy <= 0.92:\n",
    "            print(f\"SUCCESS: Best accuracy is WITHIN target range!\", flush=True)\n",
    "        elif self.best_eval_accuracy > 0.92:\n",
    "            print(f\"ABOVE TARGET: Consider using more regularization\", flush=True)\n",
    "        else:\n",
    "            print(f\"BELOW TARGET: Consider training longer or tuning hyperparameters\", flush=True)\n",
    "        \n",
    "        print(\"=\"*70, flush=True)\n",
    "        \n",
    "        return control\n",
    "\n",
    "# Initialize callback with English thresholds (use direct TRAINING_CONFIG keys)\n",
    "smart_callback = SmartTrainingCallback(\n",
    "    early_high_accuracy_threshold=TRAINING_CONFIG['early_high_accuracy_threshold'],\n",
    "    extreme_overfitting_threshold=TRAINING_CONFIG['extreme_overfitting_threshold']\n",
    ")\n",
    "\n",
    "print(\"SmartTrainingCallback initialized!\", flush=True)\n",
    "print(f\"  Early high accuracy threshold: {TRAINING_CONFIG['early_high_accuracy_threshold']} (90%)\", flush=True)\n",
    "print(f\"  Extreme overfitting threshold: {TRAINING_CONFIG['extreme_overfitting_threshold']} (95%)\", flush=True)\n",
    "print(f\"  Target accuracy range: 0.88-0.92 (88-92%)\\n\", flush=True)\n",
    "\n",
    "print(\"=\" * 70, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b9d38f",
   "metadata": {},
   "source": [
    "## Step 14: Training Arguments Configuration\n",
    "\n",
    "Configure all training parameters for English BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d906b718",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70, flush=True)\n",
    "print(\"STEP 14: TRAINING ARGUMENTS CONFIGURATION\", flush=True)\n",
    "print(\"=\"*70 + \"\\n\", flush=True)\n",
    "\n",
    "# Configure training arguments (use standalone variables from Step 2)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_SAVE_DIR,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=TRAINING_CONFIG['num_train_epochs'],\n",
    "    per_device_train_batch_size=TRAINING_CONFIG['per_device_train_batch_size'],\n",
    "    per_device_eval_batch_size=TRAINING_CONFIG['per_device_eval_batch_size'],\n",
    "    learning_rate=TRAINING_CONFIG['learning_rate'],\n",
    "    weight_decay=TRAINING_CONFIG['weight_decay'],\n",
    "    warmup_ratio=TRAINING_CONFIG['warmup_ratio'],\n",
    "    \n",
    "    # Evaluation settings\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=os.path.join(MODEL_SAVE_DIR, 'logs'),\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    \n",
    "    # Performance\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=2,\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=RANDOM_SEED,\n",
    "    \n",
    "    # Other settings\n",
    "    remove_unused_columns=False,\n",
    "    label_smoothing_factor=TRAINING_CONFIG['label_smoothing_factor'],\n",
    "    report_to=[\"tensorboard\"] if IN_COLAB else [],\n",
    ")\n",
    "\n",
    "print(\"Training Arguments Configuration:\", flush=True)\n",
    "print(f\"\\nOutput & Logging:\", flush=True)\n",
    "print(f\"  Output dir: {training_args.output_dir}\", flush=True)\n",
    "print(f\"  Logging dir: {training_args.logging_dir}\", flush=True)\n",
    "print(f\"  Logging steps: {training_args.logging_steps}\", flush=True)\n",
    "\n",
    "print(f\"\\nTraining Hyperparameters:\", flush=True)\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\", flush=True)\n",
    "print(f\"  Train batch size: {training_args.per_device_train_batch_size}\", flush=True)\n",
    "print(f\"  Eval batch size: {training_args.per_device_eval_batch_size}\", flush=True)\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\", flush=True)\n",
    "print(f\"  Weight decay: {training_args.weight_decay}\", flush=True)\n",
    "print(f\"  Warmup ratio: {training_args.warmup_ratio}\", flush=True)\n",
    "print(f\"  Label smoothing: {training_args.label_smoothing_factor}\", flush=True)\n",
    "\n",
    "print(f\"\\nEvaluation Settings:\", flush=True)\n",
    "print(f\"  Eval strategy: {training_args.eval_strategy}\", flush=True)\n",
    "print(f\"  Save strategy: {training_args.save_strategy}\", flush=True)\n",
    "print(f\"  Load best model: {training_args.load_best_model_at_end}\", flush=True)\n",
    "print(f\"  Best model metric: {training_args.metric_for_best_model}\", flush=True)\n",
    "\n",
    "print(f\"\\nPerformance:\", flush=True)\n",
    "print(f\"  FP16 (mixed precision): {training_args.fp16}\", flush=True)\n",
    "print(f\"  Dataloader workers: {training_args.dataloader_num_workers}\", flush=True)\n",
    "print(f\"  Device: {device}\", flush=True)\n",
    "\n",
    "print(f\"\\nReproducibility:\", flush=True)\n",
    "print(f\"  Seed: {training_args.seed}\\n\", flush=True)\n",
    "\n",
    "print(\"Training arguments configured successfully!\", flush=True)\n",
    "print(\"=\" * 70, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296852ae",
   "metadata": {},
   "source": [
    "## Step 15: Compute Metrics Function\n",
    "\n",
    "Define metrics computation for evaluation during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d997d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70, flush=True)\n",
    "print(\"STEP 15: COMPUTE METRICS FUNCTION\", flush=True)\n",
    "print(\"=\"*70 + \"\\n\", flush=True)\n",
    "\n",
    "# Import f1_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Define metrics computation function\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute accuracy and F1 score for evaluation\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    # Calculate F1 score (macro average)\n",
    "    f1 = f1_score(labels, predictions, average='macro')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "print(\"Compute metrics function defined!\", flush=True)\n",
    "print(\"  Metrics:\", flush=True)\n",
    "print(\"    - Accuracy (primary metric)\", flush=True)\n",
    "print(\"    - F1 Score (macro average)\\n\", flush=True)\n",
    "\n",
    "print(\"=\" * 70, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309b223c",
   "metadata": {},
   "source": [
    "## Step 16: Trainer Initialization\n",
    "\n",
    "Initialize HuggingFace Trainer with all components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1514d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70, flush=True)\n",
    "print(\"STEP 16: TRAINER INITIALIZATION\", flush=True)\n",
    "print(\"=\"*70 + \"\\n\", flush=True)\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[smart_callback]\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\", flush=True)\n",
    "print(f\"\\nTrainer Configuration:\", flush=True)\n",
    "print(f\"  Model: {MODEL_NAME}\", flush=True)\n",
    "print(f\"  Train samples: {len(train_dataset):,}\", flush=True)\n",
    "print(f\"  Eval samples: {len(val_dataset):,}\", flush=True)\n",
    "print(f\"  Callbacks: SmartTrainingCallback\", flush=True)\n",
    "print(f\"  Compute metrics: accuracy, f1\\n\", flush=True)\n",
    "\n",
    "# Calculate training steps\n",
    "total_steps = len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
    "warmup_steps = int(total_steps * training_args.warmup_ratio)\n",
    "\n",
    "print(f\"Training Steps Calculation:\", flush=True)\n",
    "print(f\"  Steps per epoch: {len(train_dataset) // training_args.per_device_train_batch_size}\", flush=True)\n",
    "print(f\"  Total epochs: {training_args.num_train_epochs}\", flush=True)\n",
    "print(f\"  Total steps: {total_steps:,}\", flush=True)\n",
    "print(f\"  Warmup steps: {warmup_steps:,} ({training_args.warmup_ratio*100:.0f}%)\\n\", flush=True)\n",
    "\n",
    "print(\"Ready to start training!\", flush=True)\n",
    "print(\"=\" * 70, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f4b8ed",
   "metadata": {},
   "source": [
    "## Step 17: Training Execution - CRITICAL\n",
    "\n",
    "Execute the training process with real-time monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada8b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70, flush=True)\n",
    "print(\"STEP 17: TRAINING EXECUTION - STARTING NOW\", flush=True)\n",
    "print(\"=\"*70 + \"\\n\", flush=True)\n",
    "\n",
    "print(\"ENGLISH BERT MODEL TRAINING\", flush=True)\n",
    "print(f\"Model: {MODEL_NAME}\", flush=True)\n",
    "print(f\"Dataset: {len(train_dataset):,} training samples\", flush=True)\n",
    "print(f\"Target Accuracy: 88-92%\", flush=True)\n",
    "print(f\"Max Epochs: {training_args.num_train_epochs}\", flush=True)\n",
    "print(f\"Early Stop Threshold: {TRAINING_CONFIG['early_high_accuracy_threshold']*100}%\\n\", flush=True)\n",
    "\n",
    "print(\"Training will automatically stop if:\", flush=True)\n",
    "print(f\"  - Accuracy reaches {TRAINING_CONFIG['early_high_accuracy_threshold']*100}% and is in target range (88-92%)\", flush=True)\n",
    "print(f\"  - Accuracy exceeds {TRAINING_CONFIG['extreme_overfitting_threshold']*100}% (overfitting prevention)\", flush=True)\n",
    "print(f\"  - Maximum epochs ({training_args.num_train_epochs}) completed\\n\", flush=True)\n",
    "\n",
    "print(\"=\"*70, flush=True)\n",
    "print(\"TRAINING IN PROGRESS...\", flush=True)\n",
    "print(\"=\"*70 + \"\\n\", flush=True)\n",
    "\n",
    "# Start training\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70, flush=True)\n",
    "    print(\"TRAINING COMPLETED SUCCESSFULLY!\", flush=True)\n",
    "    print(\"=\"*70 + \"\\n\", flush=True)\n",
    "    \n",
    "    # Display training results\n",
    "    print(\"Training Results:\", flush=True)\n",
    "    print(f\"  Final train loss: {train_result.training_loss:.4f}\", flush=True)\n",
    "    print(f\"  Total training time: {train_result.metrics.get('train_runtime', 0):.2f} seconds\", flush=True)\n",
    "    print(f\"  Samples per second: {train_result.metrics.get('train_samples_per_second', 0):.2f}\", flush=True)\n",
    "    print(f\"  Steps per second: {train_result.metrics.get('train_steps_per_second', 0):.2f}\\n\", flush=True)\n",
    "    \n",
    "    # Get evaluation metrics\n",
    "    eval_result = trainer.evaluate()\n",
    "    print(\"Final Evaluation Metrics:\", flush=True)\n",
    "    print(f\"  Eval loss: {eval_result.get('eval_loss', 0):.4f}\", flush=True)\n",
    "    print(f\"  Eval accuracy: {eval_result.get('eval_accuracy', 0):.4f} ({eval_result.get('eval_accuracy', 0)*100:.2f}%)\", flush=True)\n",
    "    print(f\"  Eval F1 score: {eval_result.get('eval_f1', 0):.4f}\\n\", flush=True)\n",
    "    \n",
    "    # Assessment against target\n",
    "    final_accuracy = eval_result.get('eval_accuracy', 0)\n",
    "    print(\"Target Assessment:\", flush=True)\n",
    "    print(f\"  Target range: 88-92%\", flush=True)\n",
    "    print(f\"  Achieved: {final_accuracy*100:.2f}%\", flush=True)\n",
    "    \n",
    "    if 0.88 <= final_accuracy <= 0.92:\n",
    "        print(\"  STATUS: SUCCESS - Within target range!\", flush=True)\n",
    "    elif final_accuracy > 0.92:\n",
    "        print(\"  STATUS: ABOVE TARGET - Excellent but may indicate overfitting\", flush=True)\n",
    "    elif final_accuracy >= 0.85:\n",
    "        print(\"  STATUS: CLOSE - Near target, acceptable performance\", flush=True)\n",
    "    else:\n",
    "        print(\"  STATUS: BELOW TARGET - Consider retraining with different hyperparameters\", flush=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70, flush=True)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nTRAINING ERROR: {str(e)}\", flush=True)\n",
    "    print(\"Please check the error message above and retry.\", flush=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e97f49",
   "metadata": {},
   "source": [
    "## Step 18: Test Set Evaluation\n",
    "\n",
    "Evaluate the trained model on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96f5b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70, flush=True)\n",
    "print(\"STEP 18: TEST SET EVALUATION\", flush=True)\n",
    "print(\"=\"*70 + \"\\n\", flush=True)\n",
    "\n",
    "print(f\"Evaluating on test set ({len(test_dataset):,} samples)...\\n\", flush=True)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(\"Test Set Results:\", flush=True)\n",
    "print(f\"  Test loss: {test_results.get('eval_loss', 0):.4f}\", flush=True)\n",
    "print(f\"  Test accuracy: {test_results.get('eval_accuracy', 0):.4f} ({test_results.get('eval_accuracy', 0)*100:.2f}%)\", flush=True)\n",
    "print(f\"  Test F1 score: {test_results.get('eval_f1', 0):.4f}\\n\", flush=True)\n",
    "\n",
    "# Get predictions for detailed analysis\n",
    "predictions_output = trainer.predict(test_dataset)\n",
    "predictions = np.argmax(predictions_output.predictions, axis=1)\n",
    "true_labels = predictions_output.label_ids\n",
    "\n",
    "print(\"Prediction Statistics:\", flush=True)\n",
    "print(f\"  Total predictions: {len(predictions):,}\", flush=True)\n",
    "print(f\"  Correct predictions: {np.sum(predictions == true_labels):,}\", flush=True)\n",
    "print(f\"  Incorrect predictions: {np.sum(predictions != true_labels):,}\\n\", flush=True)\n",
    "\n",
    "# Calculate per-class metrics\n",
    "print(\"Per-Category Performance:\", flush=True)\n",
    "print(\"-\" * 70, flush=True)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(\n",
    "    true_labels, \n",
    "    predictions, \n",
    "    target_names=[PDPL_CATEGORIES[i]['en'] for i in range(len(PDPL_CATEGORIES))],\n",
    "    digits=4\n",
    ")\n",
    "print(report, flush=True)\n",
    "\n",
    "print(\"=\" * 70, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc4e1a",
   "metadata": {},
   "source": [
    "## Step 19: Confusion Matrix & Visualization\n",
    "\n",
    "Visualize model performance with confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424ef217",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70, flush=True)\n",
    "print(\"STEP 19: CONFUSION MATRIX & VISUALIZATION\", flush=True)\n",
    "print(\"=\"*70 + \"\\n\", flush=True)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "print(\"Confusion Matrix (Test Set):\", flush=True)\n",
    "print(cm, flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=[PDPL_CATEGORIES[i]['en'][:20] for i in range(len(PDPL_CATEGORIES))],\n",
    "    yticklabels=[PDPL_CATEGORIES[i]['en'][:20] for i in range(len(PDPL_CATEGORIES))],\n",
    "    cbar_kws={'label': 'Count'}\n",
    ")\n",
    "plt.title('Confusion Matrix - English BERT PDPL Model\\nTest Set Performance', fontsize=14, pad=20)\n",
    "plt.xlabel('Predicted Category', fontsize=12)\n",
    "plt.ylabel('True Category', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save confusion matrix\n",
    "cm_path = os.path.join(TRAINING_CONFIG['model_save_dir'], 'confusion_matrix.png')\n",
    "plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Confusion matrix saved to: {cm_path}\", flush=True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "print(\"\\nPer-Category Accuracy:\", flush=True)\n",
    "print(\"-\" * 70, flush=True)\n",
    "\n",
    "for i in range(len(PDPL_CATEGORIES)):\n",
    "    class_total = np.sum(cm[i, :])\n",
    "    class_correct = cm[i, i]\n",
    "    class_accuracy = class_correct / class_total if class_total > 0 else 0\n",
    "    \n",
    "    cat_name = PDPL_CATEGORIES[i]['en']\n",
    "    print(f\"Category {i} ({cat_name}):\", flush=True)\n",
    "    print(f\"  Correct: {class_correct}/{class_total} ({class_accuracy*100:.2f}%)\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e879b426",
   "metadata": {},
   "source": [
    "## Step 20: Model Export & Packaging\n",
    "\n",
    "Export the trained model with metadata and deployment guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e35e5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70, flush=True)\n",
    "print(\"STEP 20: MODEL EXPORT & PACKAGING\", flush=True)\n",
    "print(\"=\"*70 + \"\\n\", flush=True)\n",
    "\n",
    "# Determine run number with fallback\n",
    "try:\n",
    "    from datetime import datetime\n",
    "    run_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    export_dir = f\"veriaidpo_en_run_{run_timestamp}\"\n",
    "except:\n",
    "    export_dir = \"veriaidpo_en_run_final\"\n",
    "\n",
    "export_path = os.path.join(OUTPUT_DIR, export_dir)\n",
    "\n",
    "print(f\"Exporting model to: {export_path}\\n\", flush=True)\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(export_path)\n",
    "tokenizer.save_pretrained(export_path)\n",
    "\n",
    "print(\"Model saved successfully!\", flush=True)\n",
    "print(f\"  Model files: {export_path}\", flush=True)\n",
    "\n",
    "# Save training configuration\n",
    "config_path = os.path.join(export_path, 'training_config.json')\n",
    "# Create a serializable copy of TRAINING_CONFIG\n",
    "training_config_export = {k: v for k, v in TRAINING_CONFIG.items() if isinstance(v, (str, int, float, bool, list, dict))}\n",
    "with open(config_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(training_config_export, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"  Training config: {config_path}\", flush=True)\n",
    "\n",
    "# Save test results\n",
    "results_path = os.path.join(export_path, 'test_results.json')\n",
    "test_results_dict = {\n",
    "    'test_accuracy': float(test_results.get('eval_accuracy', 0)),\n",
    "    'test_f1': float(test_results.get('eval_f1', 0)),\n",
    "    'test_loss': float(test_results.get('eval_loss', 0)),\n",
    "    'dataset_size': {\n",
    "        'train': len(train_dataset),\n",
    "        'val': len(val_dataset),\n",
    "        'test': len(test_dataset)\n",
    "    },\n",
    "    'model_name': MODEL_NAME,\n",
    "    'num_labels': num_labels\n",
    "}\n",
    "\n",
    "with open(results_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_results_dict, f, indent=2)\n",
    "\n",
    "print(f\"  Test results: {results_path}\", flush=True)\n",
    "\n",
    "# Save label mapping\n",
    "labels_path = os.path.join(export_path, 'label_mapping.json')\n",
    "with open(labels_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(PDPL_CATEGORIES, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"  Label mapping: {labels_path}\", flush=True)\n",
    "\n",
    "# Create deployment guide\n",
    "deployment_guide = f\"\"\"# VeriAIDPO English Model - Deployment Guide\n",
    "\n",
    "## Model Information\n",
    "- Model: {MODEL_NAME}\n",
    "- Language: English\n",
    "- Task: PDPL 2025 Compliance Classification\n",
    "- Accuracy: {test_results.get('eval_accuracy', 0)*100:.2f}%\n",
    "- F1 Score: {test_results.get('eval_f1', 0):.4f}\n",
    "\n",
    "## Dataset\n",
    "- Training samples: {len(train_dataset):,}\n",
    "- Validation samples: {len(val_dataset):,}\n",
    "- Test samples: {len(test_dataset):,}\n",
    "- Total samples: {len(train_dataset) + len(val_dataset) + len(test_dataset):,}\n",
    "\n",
    "## Categories (8 PDPL Categories)\n",
    "\"\"\"\n",
    "\n",
    "for cat_id, cat_data in PDPL_CATEGORIES.items():\n",
    "    deployment_guide += f\"{cat_id}. {cat_data['en']} (VN: {cat_data['vi']})\\n\"\n",
    "\n",
    "deployment_guide += f\"\"\"\n",
    "## Inference Example\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load model\n",
    "model_path = \"{export_path}\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "# Example inference\n",
    "text = \"VNG must process customer data lawfully under PDPL 2025.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "    confidence = predictions[0][predicted_class].item()\n",
    "\n",
    "print(f\"Predicted category: {{predicted_class}}\")\n",
    "print(f\"Confidence: {{confidence*100:.2f}}%\")\n",
    "```\n",
    "\n",
    "## System Requirements\n",
    "- Python 3.8+\n",
    "- PyTorch 2.0+\n",
    "- Transformers 4.30+\n",
    "- RAM: 2-4GB (inference)\n",
    "- GPU: Optional (CPU inference supported)\n",
    "\n",
    "## Performance\n",
    "- Inference time: 40-80ms per sample (CPU)\n",
    "- Batch inference: Supported\n",
    "- Max sequence length: 256 tokens\n",
    "\n",
    "## Integration with Vietnamese Model\n",
    "This English model is designed to work alongside the Vietnamese PhoBERT model for bilingual PDPL compliance detection.\n",
    "\n",
    "## Support\n",
    "For issues or questions, refer to VeriSyntra documentation.\n",
    "\"\"\"\n",
    "\n",
    "guide_path = os.path.join(export_path, 'DEPLOYMENT_GUIDE_EN.md')\n",
    "with open(guide_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(deployment_guide)\n",
    "\n",
    "print(f\"  Deployment guide: {guide_path}\\n\", flush=True)\n",
    "\n",
    "print(\"Export complete!\", flush=True)\n",
    "print(f\"\\nExported files:\", flush=True)\n",
    "print(f\"  - Model weights (pytorch_model.bin)\", flush=True)\n",
    "print(f\"  - Tokenizer files\", flush=True)\n",
    "print(f\"  - Configuration (config.json)\", flush=True)\n",
    "print(f\"  - Training config (training_config.json)\", flush=True)\n",
    "print(f\"  - Test results (test_results.json)\", flush=True)\n",
    "print(f\"  - Label mapping (label_mapping.json)\", flush=True)\n",
    "print(f\"  - Deployment guide (DEPLOYMENT_GUIDE_EN.md)\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c4253b",
   "metadata": {},
   "source": [
    "## Step 21: Create ZIP Archive for Download\n",
    "\n",
    "Package all model files into a ZIP archive for easy download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ca87b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70, flush=True)\n",
    "print(\"STEP 21: CREATE ZIP ARCHIVE FOR DOWNLOAD\", flush=True)\n",
    "print(\"=\"*70 + \"\\n\", flush=True)\n",
    "\n",
    "import shutil\n",
    "\n",
    "zip_filename = f\"{export_dir}.zip\"\n",
    "zip_path = os.path.join(TRAINING_CONFIG['output_dir'], zip_filename)\n",
    "\n",
    "print(f\"Creating ZIP archive: {zip_filename}\\n\", flush=True)\n",
    "\n",
    "# Create ZIP archive\n",
    "try:\n",
    "    shutil.make_archive(\n",
    "        base_name=os.path.join(TRAINING_CONFIG['output_dir'], export_dir),\n",
    "        format='zip',\n",
    "        root_dir=TRAINING_CONFIG['output_dir'],\n",
    "        base_dir=export_dir\n",
    "    )\n",
    "    \n",
    "    print(\"ZIP archive created successfully!\", flush=True)\n",
    "    print(f\"  File: {zip_path}\", flush=True)\n",
    "    \n",
    "    # Get ZIP file size\n",
    "    zip_size_mb = os.path.getsize(zip_path) / (1024 * 1024)\n",
    "    print(f\"  Size: {zip_size_mb:.2f} MB\\n\", flush=True)\n",
    "    \n",
    "    # List contents\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        file_list = zip_ref.namelist()\n",
    "        print(f\"ZIP archive contains {len(file_list)} files:\", flush=True)\n",
    "        for filename in file_list[:10]:\n",
    "            print(f\"  - {filename}\", flush=True)\n",
    "        if len(file_list) > 10:\n",
    "            print(f\"  ... and {len(file_list) - 10} more files\", flush=True)\n",
    "    \n",
    "    print(flush=True)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating ZIP: {str(e)}\", flush=True)\n",
    "    raise\n",
    "\n",
    "print(\"=\" * 70, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4197d763",
   "metadata": {},
   "source": [
    "## Step 22: Download Model (Google Colab Only)\n",
    "\n",
    "Download the ZIP archive directly from Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb46949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70, flush=True)\n",
    "print(\"STEP 22: DOWNLOAD MODEL (GOOGLE COLAB ONLY)\", flush=True)\n",
    "print(\"=\"*70 + \"\\n\", flush=True)\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Initiating download...\\n\", flush=True)\n",
    "    \n",
    "    try:\n",
    "        from google.colab import files\n",
    "        \n",
    "        print(f\"Downloading: {zip_filename}\", flush=True)\n",
    "        print(f\"Size: {zip_size_mb:.2f} MB\", flush=True)\n",
    "        print(\"Please wait for the download to complete...\\n\", flush=True)\n",
    "        \n",
    "        files.download(zip_path)\n",
    "        \n",
    "        print(\"Download initiated successfully!\", flush=True)\n",
    "        print(\"Check your browser's download folder.\\n\", flush=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Download error: {str(e)}\", flush=True)\n",
    "        print(f\"\\nAlternative: Access the file directly at:\", flush=True)\n",
    "        print(f\"  {zip_path}\\n\", flush=True)\n",
    "else:\n",
    "    print(\"NOT running in Google Colab.\", flush=True)\n",
    "    print(\"Model files are available at:\", flush=True)\n",
    "    print(f\"  {export_path}\", flush=True)\n",
    "    print(f\"\\nZIP archive available at:\", flush=True)\n",
    "    print(f\"  {zip_path}\", flush=True)\n",
    "    print(f\"  Size: {zip_size_mb:.2f} MB\\n\", flush=True)\n",
    "\n",
    "print(\"=\" * 70, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef36bfd6",
   "metadata": {},
   "source": [
    "## Step 23: Training Completion Summary\n",
    "\n",
    "Final summary of the entire English model training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a49d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70, flush=True)\n",
    "print(\"VERIAIDPO ENGLISH MODEL TRAINING - COMPLETION SUMMARY\", flush=True)\n",
    "print(\"=\"*70 + \"\\n\", flush=True)\n",
    "\n",
    "print(\"TRAINING PIPELINE COMPLETED SUCCESSFULLY!\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "print(\"Model Information:\", flush=True)\n",
    "print(f\"  Base Model: {model_name}\", flush=True)\n",
    "print(f\"  Language: English\", flush=True)\n",
    "print(f\"  Task: PDPL 2025 Compliance Classification\", flush=True)\n",
    "print(f\"  Categories: 8 PDPL categories\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "print(\"Dataset Statistics:\", flush=True)\n",
    "print(f\"  Total samples: {len(train_dataset) + len(val_dataset) + len(test_dataset):,}\", flush=True)\n",
    "print(f\"  Training: {len(train_dataset):,} samples (70%)\", flush=True)\n",
    "print(f\"  Validation: {len(val_dataset):,} samples (15%)\", flush=True)\n",
    "print(f\"  Test: {len(test_dataset):,} samples (15%)\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "print(\"Model Performance:\", flush=True)\n",
    "print(f\"  Test Accuracy: {test_results.get('eval_accuracy', 0)*100:.2f}%\", flush=True)\n",
    "print(f\"  Test F1 Score: {test_results.get('eval_f1', 0):.4f}\", flush=True)\n",
    "print(f\"  Target Range: 88-92%\", flush=True)\n",
    "\n",
    "final_accuracy = test_results.get('eval_accuracy', 0)\n",
    "if 0.88 <= final_accuracy <= 0.92:\n",
    "    print(f\"  Status: SUCCESS - Within target range!\", flush=True)\n",
    "elif final_accuracy > 0.92:\n",
    "    print(f\"  Status: EXCELLENT - Above target\", flush=True)\n",
    "else:\n",
    "    print(f\"  Status: Needs improvement\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "print(\"Data Integrity:\", flush=True)\n",
    "print(f\"  Template overlap: 0 (verified)\", flush=True)\n",
    "print(f\"  Company overlap: 0 (verified)\", flush=True)\n",
    "print(f\"  Data leakage: None detected\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "print(\"Exported Files:\", flush=True)\n",
    "print(f\"  Model directory: {export_path}\", flush=True)\n",
    "print(f\"  ZIP archive: {zip_path}\", flush=True)\n",
    "print(f\"  ZIP size: {zip_size_mb:.2f} MB\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "print(\"What's Included:\", flush=True)\n",
    "print(\"  - Trained BERT model (pytorch_model.bin)\", flush=True)\n",
    "print(\"  - Tokenizer files\", flush=True)\n",
    "print(\"  - Model configuration\", flush=True)\n",
    "print(\"  - Training configuration\", flush=True)\n",
    "print(\"  - Test results (JSON)\", flush=True)\n",
    "print(\"  - Label mapping (8 categories)\", flush=True)\n",
    "print(\"  - Deployment guide (DEPLOYMENT_GUIDE_EN.md)\", flush=True)\n",
    "print(\"  - Confusion matrix visualization\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "print(\"Next Steps:\", flush=True)\n",
    "print(\"  1. Download the ZIP archive (if in Colab)\", flush=True)\n",
    "print(\"  2. Extract the files locally\", flush=True)\n",
    "print(\"  3. Review DEPLOYMENT_GUIDE_EN.md for integration instructions\", flush=True)\n",
    "print(\"  4. Test inference with sample English PDPL texts\", flush=True)\n",
    "print(\"  5. Integrate with Vietnamese PhoBERT model for bilingual system\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "print(\"Bilingual System Status:\", flush=True)\n",
    "print(\"  Vietnamese Model: PhoBERT-base (6,984 samples, ~100% accuracy)\", flush=True)\n",
    "print(f\"  English Model: BERT-base-uncased (5,000 samples, {final_accuracy*100:.2f}% accuracy)\", flush=True)\n",
    "print(f\"  Combined System: 980MB total, 92-96% weighted accuracy target\", flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "print(\"=\"*70, flush=True)\n",
    "print(\"TRAINING COMPLETE - ALL STEPS EXECUTED SUCCESSFULLY!\", flush=True)\n",
    "print(\"=\"*70, flush=True)\n",
    "print(flush=True)\n",
    "\n",
    "print(\"Thank you for using VeriAIDPO English Model Training Pipeline!\", flush=True)\n",
    "print(\"For support, refer to VeriSyntra documentation.\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
